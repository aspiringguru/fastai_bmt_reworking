{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/state-farm-distracted-driver-detection  \n",
    "based on https://github.com/jonas-pettersson/fast-ai/blob/master/statefarm.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_or_prod = True #True = sample, False = production\n",
    "batch_size=8#reduced from 64 while running two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c0: safe driving  \n",
    "c1: texting - right  \n",
    "c2: talking on the phone - right  \n",
    "c3: texting - left  \n",
    "c4: talking on the phone - left  \n",
    "c5: operating the radio  \n",
    "c6: drinking  \n",
    "c7: reaching behind  \n",
    "c8: hair and makeup  \n",
    "c9: talking to passenger  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cat_desc:', <type 'list'>, 10)\n"
     ]
    }
   ],
   "source": [
    "cat_desc = ['safe driving', 'texting - right', 'talking on the phone - right', \n",
    "            'texting - left', 'talking on the phone - left', 'operating the radio', \n",
    "            'drinking', 'reaching behind', 'hair and makeup', 'talking to passenger']\n",
    "print (\"cat_desc:\", type(cat_desc), len(cat_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('HOMEPATH:', '/home/ubuntu/fastai/')\n",
      "('DATA_PATH:', '/home/ubuntu/fastai/data/state/')\n"
     ]
    }
   ],
   "source": [
    "HOMEPATH = \"/home/ubuntu/fastai/\"\n",
    "DATA_PATH = HOMEPATH + \"data/state/\"\n",
    "print(\"HOMEPATH:\", HOMEPATH)\n",
    "print(\"DATA_PATH:\", DATA_PATH)\n",
    "import os\n",
    "from __future__ import division, print_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "from shutil import copyfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING_DATA: /home/ubuntu/fastai/data/state/sample/\n",
      "WORKING_TEST: /home/ubuntu/fastai/data/state/sample/test/\n",
      "WORKING_TRAIN: /home/ubuntu/fastai/data/state/sample/train/\n",
      "WORKING_VALID: /home/ubuntu/fastai/data/state/sample/valid/\n",
      "s_or_p: _sample_\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = DATA_PATH+\"models/\"\n",
    "RESULTS_PATH = DATA_PATH+\"results/\"\n",
    "\n",
    "\n",
    "SAMPLE_DATA_PATH = DATA_PATH + \"sample/\"#choose this for testing or above for production\n",
    "SAMPLE_TEST_PATH = SAMPLE_DATA_PATH+\"test/\"\n",
    "SAMPLE_TRAIN_PATH = SAMPLE_DATA_PATH + \"train/\"\n",
    "SAMPLE_VALID_PATH = SAMPLE_DATA_PATH + \"valid/\"\n",
    "\n",
    "\n",
    "TEST_PATH = DATA_PATH+\"test/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train/\"\n",
    "VALID_PATH = DATA_PATH + \"valid/\"\n",
    "\n",
    "\n",
    "if sample_or_prod:\n",
    "    WORKING_DATA  = SAMPLE_DATA_PATH\n",
    "    WORKING_TEST  = SAMPLE_TEST_PATH\n",
    "    WORKING_TRAIN = SAMPLE_TRAIN_PATH\n",
    "    WORKING_VALID = SAMPLE_VALID_PATH\n",
    "    s_or_p = \"_sample_\"\n",
    "else:\n",
    "    WORKING_DATA  = DATA_PATH\n",
    "    WORKING_TEST  = TEST_PATH\n",
    "    WORKING_TRAIN = TRAIN_PATH\n",
    "    WORKING_VALID = VALID_PATH\n",
    "    s_or_p = \"_prod_\"\n",
    "\n",
    "    \n",
    "print (\"WORKING_DATA:\", WORKING_DATA)\n",
    "print (\"WORKING_TEST:\", WORKING_TEST)\n",
    "print (\"WORKING_TRAIN:\", WORKING_TRAIN)\n",
    "print (\"WORKING_VALID:\", WORKING_VALID)\n",
    "print (\"s_or_p:\", s_or_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai/data/state\n",
      "\u001b[01;34m.\u001b[00m\n",
      "├── \u001b[01;34mmodels\u001b[00m\n",
      "├── \u001b[01;34mresults\u001b[00m\n",
      "├── \u001b[01;34msample\u001b[00m\n",
      "│   ├── \u001b[01;34mtest\u001b[00m\n",
      "│   │   └── \u001b[01;34munknown\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc0\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc1\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc2\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc3\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc4\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc5\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc6\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc7\u001b[00m\n",
      "│   │   ├── \u001b[01;34mc8\u001b[00m\n",
      "│   │   └── \u001b[01;34mc9\u001b[00m\n",
      "│   └── \u001b[01;34mvalid\u001b[00m\n",
      "│       ├── \u001b[01;34mc0\u001b[00m\n",
      "│       ├── \u001b[01;34mc1\u001b[00m\n",
      "│       ├── \u001b[01;34mc2\u001b[00m\n",
      "│       ├── \u001b[01;34mc3\u001b[00m\n",
      "│       ├── \u001b[01;34mc4\u001b[00m\n",
      "│       ├── \u001b[01;34mc5\u001b[00m\n",
      "│       ├── \u001b[01;34mc6\u001b[00m\n",
      "│       ├── \u001b[01;34mc7\u001b[00m\n",
      "│       ├── \u001b[01;34mc8\u001b[00m\n",
      "│       └── \u001b[01;34mc9\u001b[00m\n",
      "├── \u001b[01;34mtest\u001b[00m\n",
      "│   └── \u001b[01;34munknown\u001b[00m\n",
      "├── \u001b[01;34mtrain\u001b[00m\n",
      "│   ├── \u001b[01;34mc0\u001b[00m\n",
      "│   ├── \u001b[01;34mc1\u001b[00m\n",
      "│   ├── \u001b[01;34mc2\u001b[00m\n",
      "│   ├── \u001b[01;34mc3\u001b[00m\n",
      "│   ├── \u001b[01;34mc4\u001b[00m\n",
      "│   ├── \u001b[01;34mc5\u001b[00m\n",
      "│   ├── \u001b[01;34mc6\u001b[00m\n",
      "│   ├── \u001b[01;34mc7\u001b[00m\n",
      "│   ├── \u001b[01;34mc8\u001b[00m\n",
      "│   └── \u001b[01;34mc9\u001b[00m\n",
      "└── \u001b[01;34mvalid\u001b[00m\n",
      "    ├── \u001b[01;34mc0\u001b[00m\n",
      "    ├── \u001b[01;34mc1\u001b[00m\n",
      "    ├── \u001b[01;34mc2\u001b[00m\n",
      "    ├── \u001b[01;34mc3\u001b[00m\n",
      "    ├── \u001b[01;34mc4\u001b[00m\n",
      "    ├── \u001b[01;34mc5\u001b[00m\n",
      "    ├── \u001b[01;34mc6\u001b[00m\n",
      "    ├── \u001b[01;34mc7\u001b[00m\n",
      "    ├── \u001b[01;34mc8\u001b[00m\n",
      "    └── \u001b[01;34mc9\u001b[00m\n",
      "\n",
      "51 directories\n"
     ]
    }
   ],
   "source": [
    "os.chdir(DATA_PATH)\n",
    "print (os.getcwd())\n",
    "!tree -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirFileList(dir_path):\n",
    "    return [name for name in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, name))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORIES: ['c0/', 'c1/', 'c2/', 'c3/', 'c4/', 'c5/', 'c6/', 'c7/', 'c8/', 'c9/']\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = []\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    CATEGORIES.append(\"c\"+str(i)+\"/\")\n",
    "print (\"CATEGORIES:\", CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN = 'unknown/'\n",
    "\n",
    "DATA_DIR_LIST = [DATA_PATH, SAMPLE_DATA_PATH, SAMPLE_TEST_PATH, SAMPLE_TEST_PATH+UNKNOWN, SAMPLE_TRAIN_PATH, \n",
    "                 SAMPLE_VALID_PATH, TEST_PATH, TEST_PATH+UNKNOWN, TRAIN_PATH, VALID_PATH, MODEL_PATH, RESULTS_PATH]\n",
    "\n",
    "\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    DATA_DIR_LIST.append(TRAIN_PATH+category)\n",
    "    DATA_DIR_LIST.append(VALID_PATH+category)\n",
    "    DATA_DIR_LIST.append(SAMPLE_TRAIN_PATH+category)\n",
    "    DATA_DIR_LIST.append(SAMPLE_VALID_PATH+category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showLayersInfo(model):\n",
    "    print (\"Number of layers : \", len(model.layers))\n",
    "    count = 0\n",
    "    for layer in model.layers:\n",
    "        print (count, type(layer), \", trainable:\", layer.trainable)\n",
    "        print (\"input:\", layer.input_shape, \", output:\",layer.output_shape, \", len(weights)\", len(layer.get_weights()), \"\\n\")\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validate'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validate'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listDirsFileCount(DATA_DIR_LIST):\n",
    "    DATA_DIR_LIST = sorted(DATA_DIR_LIST)\n",
    "    for dir_ in DATA_DIR_LIST:\n",
    "        print (dir_, len(dirFileList(dir_)))\n",
    "        \n",
    "#NB: could have used global DATA_DIR_LIST  to access global variable DATA_DIR_LIST \n",
    "# without having to pass DATA_DIR_LIST as an arg, but this gives more flexability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai/data/state/ 5\n",
      "/home/ubuntu/fastai/data/state/models/ 9\n",
      "/home/ubuntu/fastai/data/state/results/ 2\n",
      "/home/ubuntu/fastai/data/state/sample/ 0\n",
      "/home/ubuntu/fastai/data/state/sample/test/ 0\n",
      "/home/ubuntu/fastai/data/state/sample/test/unknown/ 15945\n",
      "/home/ubuntu/fastai/data/state/sample/train/ 0\n",
      "/home/ubuntu/fastai/data/state/sample/train/c0/ 347\n",
      "/home/ubuntu/fastai/data/state/sample/train/c1/ 314\n",
      "/home/ubuntu/fastai/data/state/sample/train/c2/ 323\n",
      "/home/ubuntu/fastai/data/state/sample/train/c3/ 325\n",
      "/home/ubuntu/fastai/data/state/sample/train/c4/ 323\n",
      "/home/ubuntu/fastai/data/state/sample/train/c5/ 322\n",
      "/home/ubuntu/fastai/data/state/sample/train/c6/ 324\n",
      "/home/ubuntu/fastai/data/state/sample/train/c7/ 278\n",
      "/home/ubuntu/fastai/data/state/sample/train/c8/ 268\n",
      "/home/ubuntu/fastai/data/state/sample/train/c9/ 298\n",
      "/home/ubuntu/fastai/data/state/sample/valid/ 0\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c0/ 153\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c1/ 138\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c2/ 140\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c3/ 145\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c4/ 141\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c5/ 140\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c6/ 143\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c7/ 121\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c8/ 117\n",
      "/home/ubuntu/fastai/data/state/sample/valid/c9/ 131\n",
      "/home/ubuntu/fastai/data/state/test/ 0\n",
      "/home/ubuntu/fastai/data/state/test/unknown/ 79726\n",
      "/home/ubuntu/fastai/data/state/train/ 0\n",
      "/home/ubuntu/fastai/data/state/train/c0/ 1740\n",
      "/home/ubuntu/fastai/data/state/train/c1/ 1585\n",
      "/home/ubuntu/fastai/data/state/train/c2/ 1623\n",
      "/home/ubuntu/fastai/data/state/train/c3/ 1641\n",
      "/home/ubuntu/fastai/data/state/train/c4/ 1625\n",
      "/home/ubuntu/fastai/data/state/train/c5/ 1618\n",
      "/home/ubuntu/fastai/data/state/train/c6/ 1625\n",
      "/home/ubuntu/fastai/data/state/train/c7/ 1402\n",
      "/home/ubuntu/fastai/data/state/train/c8/ 1336\n",
      "/home/ubuntu/fastai/data/state/train/c9/ 1489\n",
      "/home/ubuntu/fastai/data/state/valid/ 0\n",
      "/home/ubuntu/fastai/data/state/valid/c0/ 749\n",
      "/home/ubuntu/fastai/data/state/valid/c1/ 682\n",
      "/home/ubuntu/fastai/data/state/valid/c2/ 694\n",
      "/home/ubuntu/fastai/data/state/valid/c3/ 705\n",
      "/home/ubuntu/fastai/data/state/valid/c4/ 701\n",
      "/home/ubuntu/fastai/data/state/valid/c5/ 694\n",
      "/home/ubuntu/fastai/data/state/valid/c6/ 700\n",
      "/home/ubuntu/fastai/data/state/valid/c7/ 600\n",
      "/home/ubuntu/fastai/data/state/valid/c8/ 575\n",
      "/home/ubuntu/fastai/data/state/valid/c9/ 640\n"
     ]
    }
   ],
   "source": [
    "listDirsFileCount(DATA_DIR_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory: /home/ubuntu/fastai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5110)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(HOMEPATH)\n",
    "print (\"current working directory:\", os.getcwd())\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "#path = \"data/state/\"\n",
    "path = \"data/state/sample/\"\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3122 images belonging to 10 classes.\n",
      "Found 1369 images belonging to 10 classes.\n",
      "Found 15945 images belonging to 1 classes.\n",
      "/home/ubuntu/fastai/data/state/sample/train/ <class 'keras.preprocessing.image.DirectoryIterator'>\n",
      "/home/ubuntu/fastai/data/state/sample/valid/ <class 'keras.preprocessing.image.DirectoryIterator'>\n",
      "/home/ubuntu/fastai/data/state/sample/test/ <class 'keras.preprocessing.image.DirectoryIterator'>\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(WORKING_TRAIN, batch_size=batch_size)\n",
    "val_batches = get_batches(WORKING_VALID, batch_size=batch_size*2, shuffle=False)\n",
    "test_batches = get_batches(WORKING_TEST, batch_size=1, shuffle=False)\n",
    "print (WORKING_TRAIN, type(batches))\n",
    "print (WORKING_VALID, type(val_batches))\n",
    "print (WORKING_TEST, type(test_batches))\n",
    "\n",
    "\n",
    "#https://keras.io/preprocessing/image/\n",
    "#https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/DirectoryIterator\n",
    "#NB: utils.get_batches has default target_size=(224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3122 images belonging to 10 classes.\n",
      "Found 1369 images belonging to 10 classes.\n",
      "Found 15945 images belonging to 1 classes.\n",
      "val_classes <type 'numpy.ndarray'> (1369,)\n",
      "trn_classes <type 'numpy.ndarray'> (3122,)\n",
      "val_labels <type 'numpy.ndarray'> (1369, 10)\n",
      "trn_labels <type 'numpy.ndarray'> (3122, 10)\n",
      "val_filenames <type 'list'> 1369\n",
      "filenames <type 'list'> 3122\n",
      "test_filename <type 'list'> 15945\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames, test_filename) = get_classes(WORKING_DATA)\n",
    "print (\"val_classes\", type(val_classes), val_classes.shape)\n",
    "print (\"trn_classes\", type(trn_classes), trn_classes.shape)\n",
    "print (\"val_labels\", type(val_labels), val_labels.shape)\n",
    "print (\"trn_labels\", type(trn_labels), trn_labels.shape)\n",
    "print (\"val_filenames\", type(val_filenames), len(val_filenames))\n",
    "print (\"filenames\", type(filenames), len(filenames))\n",
    "print (\"test_filename\", type(test_filename), len(test_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_filename[0] unknown/img_92913.jpg\n"
     ]
    }
   ],
   "source": [
    "print (\"test_filename[0]\", test_filename[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_classes: <type 'numpy.int32'> 0 [0 1 2 3 4 5 6 7 8 9]\n",
      "trn_classes: <type 'numpy.int32'> 0 [0 1 2 3 4 5 6 7 8 9]\n",
      "val_labels: <type 'numpy.ndarray'> [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  1.]\n",
      "trn_labels: <type 'numpy.ndarray'> [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print (\"val_classes:\", type(val_classes[0]), val_classes[0], np.unique(val_classes))\n",
    "print (\"trn_classes:\", type(trn_classes[0]), trn_classes[0], np.unique(trn_classes))\n",
    "print (\"val_labels:\", type(val_labels[0]), val_labels[0], np.unique(val_labels))\n",
    "print (\"trn_labels:\", type(trn_labels[0]), trn_labels[0], np.unique(trn_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_path: /home/ubuntu/fastai/data/state/train/c0/\n",
      "image_sizes: set([(480, 640, 3)])\n",
      "No. of disctinct image sizes: 1\n"
     ]
    }
   ],
   "source": [
    "temp_path = TRAIN_PATH+CATEGORIES[0]\n",
    "print (\"temp_path:\", temp_path)\n",
    "files = dirFileList(temp_path)\n",
    "\n",
    "image_sizes = set([])\n",
    "for i in range(len(files)):\n",
    "    img_shape = cv2.imread(temp_path+files[i]).shape\n",
    "    #print (\"i:\", i, \":\", img_shape)\n",
    "    image_sizes.add(img_shape)\n",
    "print (\"image_sizes:\", image_sizes)\n",
    "print (\"No. of disctinct image sizes:\", len(image_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3122 images belonging to 10 classes.\n",
      "Found 1369 images belonging to 10 classes.\n",
      "Found 15945 images belonging to 1 classes.\n",
      "trn: <type 'numpy.ndarray'>\n",
      "val: <type 'numpy.ndarray'>\n",
      "test: <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(WORKING_TRAIN)\n",
    "val = get_data(WORKING_VALID)\n",
    "test = get_data(WORKING_TEST)\n",
    "print (\"trn:\", type(trn))\n",
    "print (\"val:\", type(val))\n",
    "print (\"test:\", type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn: <type 'numpy.ndarray'>\n",
      "val: <type 'numpy.ndarray'>\n",
      "test: <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print (\"trn:\", type(trn))\n",
    "print (\"val:\", type(val))\n",
    "print (\"test:\", type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "model=vgg.model\n",
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_conv_idx: 30\n"
     ]
    }
   ],
   "source": [
    "print (\"last_conv_idx:\", last_conv_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next three blocks are pre-computing the output of conv_layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startTime: 2018-01-05 01:24:05.478651\n",
      "Time elapsed (hh:mm:ss.ms) 0:02:39.023115\n"
     ]
    }
   ],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "conv_feat = conv_model.predict_generator(batches, batches.nb_sample)\n",
    "#batches = get_batches(WORKING_TRAIN, batch_size=batch_size) : Found 3122 images belonging to 10 classes.\n",
    "print (\"conv_feat:\", type(conv_feat))\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startTime: 2018-01-05 01:26:44.514200\n",
      "Time elapsed (hh:mm:ss.ms) 0:01:03.444365\n"
     ]
    }
   ],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "conv_val_feat = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "#val_batches = get_batches(WORKING_VALID, batch_size=batch_size*2, shuffle=False):Found 1369 images belonging to 10 classes.\n",
    "print (\"conv_val_feat:\", type(conv_val_feat))\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startTime: 2018-01-05 01:27:47.970731\n",
      "Time elapsed (hh:mm:ss.ms) 0:24:44.554337\n"
     ]
    }
   ],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "conv_test_feat = conv_model.predict_generator(test_batches, test_batches.nb_sample)\n",
    "#test_batches = get_batches(WORKING_TEST, batch_size=1, shuffle=False) : Found 15945 images belonging to 1 classes.\n",
    "print (\"conv_test_feat:\", type(conv_test_feat))\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape from conv_layers:  (512, 14, 14)\n"
     ]
    }
   ],
   "source": [
    "print (\"output shape from conv_layers: \", conv_layers[-1].output_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    #create list of layers to accept input from conv_layers model.\n",
    "    #output layer is Dense with 10 categories.\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3122 samples, validate on 1369 samples\n",
      "Epoch 1/1\n",
      "3122/3122 [==============================] - 6s - loss: 4.2499 - acc: 0.1006 - val_loss: 2.5655 - val_acc: 0.1030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc349582490>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=1, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3122 samples, validate on 1369 samples\n",
      "Epoch 1/2\n",
      "3122/3122 [==============================] - 6s - loss: 3.2319 - acc: 0.1015 - val_loss: 2.3709 - val_acc: 0.0920\n",
      "Epoch 2/2\n",
      "3122/3122 [==============================] - 7s - loss: 2.6921 - acc: 0.1166 - val_loss: 2.3243 - val_acc: 0.1059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc349582950>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, nb_epoch=2, \n",
    "             validation_data=(conv_val_feat, val_labels))\n",
    "#very low accuracy returned on sample: loss: 3.2319 - acc: 0.1015 - val_loss: 2.3709 - val_acc: 0.0920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bn_model.save_weights(MODEL_PATH+'conv8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now precompute data augmentation and perform dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3122 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "da_batches = get_batches(path+'train', gen_t, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-43291889102f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mda_conv_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mda_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sample\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                                             \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                                             \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                                             pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                         \u001b[0mall_outs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "da_conv_feat = conv_model.predict_generator(da_batches, da_batches.nb_sample*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_array(MODEL_PATH+'da_conv_feat2.dat', da_conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#da_conv_feat = load_array(MODEL_PATH+'results/da_conv_feat2.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the real training data as well in its non-augmented form.\n",
    "da_conv_feat = np.concatenate([da_conv_feat, conv_feat])\n",
    "print (\"da_conv_feat:\", type(da_conv_feat), da_conv_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the labels 6 times too. \n",
    "#NB: assumes the augmentation process replicates the original data abc abc abc not aaa bbb ccc\n",
    "da_trn_labels = np.concatenate([trn_labels]*6)\n",
    "print (\"da_trn_labels:\", type(da_trn_labels), da_trn_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_da_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
