{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "HOMEPATH = \"/home/ubuntu/fastai/\"\n",
    "DATA_HOME_DIR = HOMEPATH + \"data/cifar-10/\"\n",
    "DATA_WORKING = DATA_HOME_DIR# + \"sample/\"\n",
    "DATA_WORKING_RESULTS = DATA_WORKING + \"results/\"\n",
    "\n",
    "print (\"DATA_WORKING      :\", DATA_WORKING)\n",
    "print (\"DATA_WORKING_RESULTS:\", DATA_WORKING_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model \n",
    "import os\n",
    "\n",
    "print(\"----------------\")\n",
    "def set_keras_backend(backend):\n",
    "\n",
    "    if K.backend() != backend:\n",
    "        os.environ['KERAS_BACKEND'] = backend\n",
    "        reload(K)\n",
    "        assert K.backend() == backend\n",
    "\n",
    "set_keras_backend(\"theano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(HOMEPATH)\n",
    "print (\"os.getcwd:\", os.getcwd())\n",
    "# Rather than importing everything manually, we'll make things easy\n",
    "#   and load them all in utils.py, and just import them from there.\n",
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from utils import plots, get_batches, plot_confusion_matrix, get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(HOMEPATH)\n",
    "print (\"os.getcwd:\", os.getcwd())\n",
    "%matplotlib inline\n",
    "from __future__ import division,print_function\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from random import shuffle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import cPickle as pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from vgg16 import Vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = HOMEPATH + \"data/dogscats/sample/\"\n",
    "path = DATA_HOME_DIR \n",
    "#path = DATA_HOME_DIR + \"sample/\"\n",
    "train_path = path + 'train/'\n",
    "test_path = path + 'test/'\n",
    "testUnknown_path = path + 'test/unknown/'\n",
    "valid_path = path + 'valid/'\n",
    "model_path = path + 'models/'\n",
    "sample_path = path + 'sample/'\n",
    "sampleTrain_path = path + 'sample/train/'\n",
    "sampleTest_path = path + 'sample/test/'\n",
    "sampleValid_path = path + 'sample/valid/'\n",
    "sampleResults_path = path + 'sample/results/'\n",
    "sampleTestUnknown_path = path + 'sample/test/unknown/'\n",
    "\n",
    "categories = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirFileList(dir_path):\n",
    "    return [name for name in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, name))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DATA_HOME_DIR)\n",
    "!tree -d\n",
    "#should display an ascii tree representation of directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirList = [train_path, valid_path, sampleTrain_path, sampleValid_path]\n",
    "\n",
    "for dir_path in dirList:\n",
    "    for category in categories:\n",
    "        print (\"# of files in\", dir_path+category, \":\", len(dirFileList(dir_path+category)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = Vgg16()\n",
    "model = vgg.model\n",
    "print (\"type(vgg):\", type(vgg))\n",
    "print (\"type(model):\", type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showLayersInfo(model):\n",
    "    print (\"Number of layers : \", len(model.layers))\n",
    "    for layer in model.layers:\n",
    "        print (type(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showLayersInfo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA_WORKING:\", DATA_WORKING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use batch size of 1 since we're just doing preprocessing on the CPU\n",
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "#nb: get_batches is from utils.get_batches\n",
    "val_batches = get_batches(DATA_WORKING+'valid/', shuffle=False, batch_size=1)\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('val_batches: Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "batches     = get_batches(DATA_WORKING+'train/', shuffle=False, batch_size=1)\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('batches : Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"val_batches:\", type(val_batches))\n",
    "print (\"batches:\", type(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname):\n",
    "    print (\"load_array:fname:\", fname)\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"DATA_WORKING:\", DATA_WORKING)\n",
    "#nb: get_data is from utils.get_data\n",
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "val_data = get_data(DATA_WORKING+'valid')\n",
    "\n",
    "print (\"val_data:\", type(val_data), val_data.shape)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('val_data:Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"DATA_WORKING:\", DATA_WORKING)\n",
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "trn_data = get_data(DATA_WORKING+'train')\n",
    "print (\"trn_data:\", type(trn_data), trn_data.shape)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"DATA_WORKING_RESULTS:\", DATA_WORKING_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.save_array uses bcolz. very small filesize generated.\n",
    "save_array(DATA_WORKING_RESULTS+'train_data_full.bc', trn_data)\n",
    "save_array(DATA_WORKING_RESULTS+'valid_data_full.bc', val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"DATA_WORKING_RESULTS:\", DATA_WORKING_RESULTS)\n",
    "print (DATA_WORKING_RESULTS+'train_data.bc')\n",
    "#trn_data = load_array(DATA_WORKING_RESULTS+'train_data_full.bc')\n",
    "#val_data = load_array(DATA_WORKING_RESULTS+'valid_data_full.bc')\n",
    "print (\"trn_data:\", type(trn_data), trn_data.shape)#should return trn_data: <type 'numpy.ndarray'> (8000, 3, 224, 224)\n",
    "print (\"val_data:\", type(val_data), val_data.shape)#should return val_data: <type 'numpy.ndarray'> (2000, 3, 224, 224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras returns *classes* as a single column, so we convert to one hot encoding\n",
    "def onehot(x): \n",
    "    return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "print (\"val_classes:\", type(val_classes), np.unique(val_classes), val_classes.shape)\n",
    "print(\"val_classes[0:10]:\", val_classes[0:10])\n",
    "print (\"trn_classes:\", type(trn_classes), np.unique(val_classes), trn_classes.shape)\n",
    "print(\"trn_classes[0:10]:\", trn_classes[0:10])\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)\n",
    "print (\"after onehot conversion.\")\n",
    "print (\"val_labels:\", val_labels[0])\n",
    "print (\"trn_labels:\", trn_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"start\")\n",
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "trn_features = model.predict(trn_data, batch_size=batch_size)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "val_features = model.predict(val_data, batch_size=batch_size)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"trn_features:\", trn_features.shape)\n",
    "print (\"val_features:\", val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"DATA_WORKING_RESULTS:\",DATA_WORKING_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(DATA_WORKING_RESULTS+'train_lastlayer_features_full.bc', trn_features)\n",
    "save_array(DATA_WORKING_RESULTS+'valid_lastlayer_features_full.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trn_features = load_array(DATA_WORKING_RESULTS+'train_lastlayer_features.bc')\n",
    "#val_features = load_array(DATA_WORKING_RESULTS+'valid_lastlayer_features.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"trn_features:\", type(trn_features), trn_features.shape)\n",
    "print (\"val_features:\", type(val_features), val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 inputs, since trn_features.shape[1]=1000, and 10 outputs, for # of categories\n",
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "\n",
    "lm = Sequential([ Dense(10, activation='softmax', input_shape=(1000,)) ])\n",
    "lm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"lm:\", type(lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(categories), len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "lm.fit(trn_features, trn_labels, nb_epoch=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels))\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(lm))\n",
    "print (lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "preds = lm.predict_classes(val_features, batch_size=batch_size)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(preds), preds.shape)\n",
    "print (\"preds[0:10]:\", preds[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "#probabilities of each category results in numpy array with column for each category.\n",
    "probs = lm.predict_proba(val_features, batch_size=batch_size)#[:,0]\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "\n",
    "print (\"probs:\", type(probs), probs.shape)\n",
    "print (probs[0,])\n",
    "print ( [\"%.4f\" % v for v in probs[0,]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = val_batches.filenames\n",
    "print (\"filenames:\", type(filenames), len(filenames))\n",
    "print (filenames[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images to view for each visualization task\n",
    "n_view = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesFromFilenames(filenames):\n",
    "    #returns text versions of categories from filenames.\n",
    "    #requires filenames in format of 'category-text-form/123456.png'\n",
    "    #strips out the category-text-form prior to '/'\n",
    "    classes = []\n",
    "    #print (\"classesFromFilenames:idx:\", idx)\n",
    "    for i in range(0, len(filenames)):\n",
    "        pos = filenames[i].find('/')\n",
    "        #print (\"pos:\", pos, filenames[i][0:pos])\n",
    "        classes.append(filenames[i][0:pos])\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classesFromPreds(preds, categories):\n",
    "    #returns text versions of categories\n",
    "    categs = []\n",
    "    for pred in preds:\n",
    "        #print(\"pred:\", pred, categories[pred])\n",
    "        categs.append(categories[pred])\n",
    "    return categs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots_idx(path, idx, titles=None):\n",
    "    #titles = numpy array idx rows x 10 columns, need to select highest prob from each row.\n",
    "    maxProb = []\n",
    "    for i in range(0, titles.shape[0]):\n",
    "        #print (titles[i,], type(titles[i,]))\n",
    "        #print (np.argmax(titles[i,]))\n",
    "        maxProb.append(titles[i,np.argmax(titles[i,])])\n",
    "    plots([image.load_img(path + filenames[i]) for i in idx], titles=maxProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categsFromFileNames = classesFromFilenames(filenames)\n",
    "print(\"categsFromFileNames:\", len(categsFromFileNames), categsFromFileNames[0:4])\n",
    "categsFromPreds = classesFromPreds(preds, categories)\n",
    "print(\"categsFromPreds:\", len(categsFromPreds), categsFromPreds[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotSamples(matchIndex):\n",
    "    #numpy.random.permutation\n",
    "    idx = permutation(matchIndex)[:n_view]\n",
    "    print (\"idx:\", idx)\n",
    "    for i in idx:\n",
    "        print(filenames[i])\n",
    "\n",
    "    #print (classesFromFilenames(idx, filenames))\n",
    "    #print (\"probs[idx].shape:\", probs[idx].shape)\n",
    "    plots_idx(DATA_WORKING+'valid/', idx, probs[idx])\n",
    "    #print (\"idx:\", idx)\n",
    "    print (\"actual from filenames:\", [categsFromFileNames[i] for i in idx])\n",
    "    print (\"predicted            :\",     [categsFromPreds[i] for i in idx])\n",
    "    #print (\"probs[idx[0],]:\", probs[idx[0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select correctly predicted.\n",
    "matchIndex = []\n",
    "for i in range(0,len(categsFromFileNames)):\n",
    "    if categsFromFileNames[i]==categsFromPreds[i]:\n",
    "        matchIndex.append(i)\n",
    "print (\"# of correctly predicted : matchIndex:\", len(matchIndex))\n",
    "plotSamples(matchIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select incorrectly predicted.\n",
    "matchIndex = []\n",
    "for i in range(0,len(categsFromFileNames)):\n",
    "    if categsFromFileNames[i]!=categsFromPreds[i]:\n",
    "        matchIndex.append(i)\n",
    "print (\"# of incorrectly predicted : matchIndex:\", len(matchIndex))\n",
    "plotSamples(matchIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_classes, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"type(val_batches.class_indices):\", type(val_batches.class_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(12,12)) #does not affect size, method is inside utils\n",
    "\n",
    "plot_confusion_matrix(cm, val_batches.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"before pop, # of layers:\", len(model.layers))\n",
    "if len(model.layers)==38:\n",
    "    print (\"# of layers == 38., poping top layer.\")\n",
    "    model.pop()\n",
    "else:\n",
    "    print (\"# of layers not == 38.\")\n",
    "print (\"after pop, # of layers:\", len(model.layers))\n",
    "for layer in model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(vgg.model))\n",
    "#not really useful, here for reference only\n",
    "#for layer in model.layers:\n",
    "#    print(layer.name, layer.inbound_nodes, layer.outbound_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation='softmax'))#NB: Dense requires 10 because 10 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"after adding dense layer, # of layers:\", len(model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen=image.ImageDataGenerator()\n",
    "print (\"gen:\", type(gen))\n",
    "batches = gen.flow(trn_data, trn_labels, batch_size=batch_size, shuffle=True)\n",
    "#https://keras.io/preprocessing/image/\n",
    "#flow(x, y): Takes numpy data & label arrays, \n",
    "#and generates batches of augmented/normalized data. \n",
    "#Yields batches indefinitely, in an infinite loop.\n",
    "\n",
    "val_batches = gen.flow(val_data, val_labels, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print (\"batches:\", type(batches))\n",
    "print (\"val_batches:\", type(val_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, batches, val_batches, nb_epoch=1):\n",
    "    model.fit_generator(batches, \n",
    "                        samples_per_epoch=batches.n, \n",
    "                        nb_epoch=nb_epoch, \n",
    "                        validation_data=val_batches, \n",
    "                        nb_val_samples=val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.1)\n",
    "#https://keras.io/optimizers/#rmsprop\n",
    "#This optimizer is usually a good choice for recurrent neural networks.\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime= datetime.now()\n",
    "fit_model(model, batches, val_batches, nb_epoch=2)\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"DATA_WORKING_RESULTS:\", DATA_WORKING_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(DATA_WORKING_RESULTS+'finetune1_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(DATA_WORKING_RESULTS+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"type(model):\", type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_classes(val_data, batch_size=batch_size)\n",
    "probs = model.predict_proba(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"preds:\", preds.shape)\n",
    "print(\"probs:\", probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[:8,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(val_classes, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"type(val_batches):\", type(val_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#recall - val_batches was generated from loading files. possibly need to initialize again?\n",
    "#val_batches = get_batches(DATA_WORKING+'valid/', shuffle=False, batch_size=1)\n",
    "#plot_confusion_matrix(cm, val_batches.class_indices)\n",
    "#AttributeError: 'NumpyArrayIterator' object has no attribute 'class_indices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print (type(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "# Get the index of the first dense layer...\n",
    "first_dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "print (type(first_dense_idx), first_dense_idx)\n",
    "# ...and set this and all subsequent layers to trainable\n",
    "for layer in layers[first_dense_idx:]: \n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(K)\n",
    "#recall from above: from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_value(opt.lr, 0.01)\n",
    "#https://keras.io/backend/\n",
    "#keras.backend.set_value(x, value)\n",
    "#https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_value\n",
    "\n",
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "fit_model(model, batches, val_batches, 3)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(DATA_WORKING_RESULTS+'finetune2_.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers[12:]: layer.trainable=True\n",
    "K.set_value(opt.lr, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(model, batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(DATA_WORKING_RESULTS+'finetune3_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model #move this to top when run complete.\n",
    "model.save(DATA_WORKING_RESULTS+'CIFAR-10_part2_final_model_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this did not work.\n",
    "def save_object(obj, filename):\n",
    "    filename = DATA_WORKING_RESULTS+filename\n",
    "    with open(filename, 'wb') as output:\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "    print (\"method save_object loaded with DATA_WORKING_RESULTS=\", DATA_WORKING_RESULTS)\n",
    "# sample usage\n",
    "#save_object(company1, 'company1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_object(model, 'CIFAR-10_part2_final_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_model = pickle.load( open( DATA_WORKING_RESULTS+'CIFAR-10_part2_final_model.pkl', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(type(pickle_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = load_array(DATA_WORKING_RESULTS+'valid_lastlayer_features.bc')\n",
    "print (\"val_features:\", type(val_features), val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "preds = model.predict_classes(val_features, batch_size=batch_size)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
