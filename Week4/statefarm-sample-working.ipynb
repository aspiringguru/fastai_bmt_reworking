{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter State Farm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5110)\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (keras.__version__)\n",
    "#print (theano.__version__)\n",
    "#print (tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "for m in pip.get_installed_distributions():\n",
    "    if m.project_name == 'keras':\n",
    "        print(m.version)\n",
    "print (\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set FIXED variables\n",
    "HOMEPATH = \"/home/ubuntu/fastai/\"\n",
    "VALIDPATH = HOMEPATH + 'data/state/valid/'\n",
    "TRAINPATH = HOMEPATH + 'data/state/train/'\n",
    "TESTPATH = HOMEPATH + 'data/state/test/'\n",
    "\n",
    "SAMPLEPATH = HOMEPATH + 'data/state/sample/'\n",
    "SAMPLEVALIDPATH = HOMEPATH + 'data/state/sample/valid/'\n",
    "SAMPLETRAINPATH = HOMEPATH + 'data/state/sample/train/'\n",
    "SAMPLETESTPATH = HOMEPATH + 'data/state/sample/test/'\n",
    "\n",
    "\n",
    "import os, errno\n",
    "\n",
    "from datetime import datetime\n",
    "from shutil import copyfile, move\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('current working directory:', '/home/ubuntu/fastai')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(HOMEPATH)\n",
    "print (\"current working directory:\", os.getcwd())\n",
    "\n",
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "#path = \"data/state/\"\n",
    "path = \"data/state/sample/\"\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following assumes you've already created your validation set - remember that the training and validation set should contain *different drivers*, as mentioned on the Kaggle competition page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "HOLD - markdown cell  \n",
    "%cd data/state  \n",
    "%cd train  \n",
    "%mkdir ../sample  \n",
    "%mkdir ../sample/train  \n",
    "%mkdir ../sample/valid  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#after unpacking files from kaggle data download. dirs and files already exist.\n",
    "\n",
    "DIR = TESTPATH\n",
    "print (DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n",
    "79726/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# dir and files already exist after unpacking kaggle data.\n",
    "for i in range(0, 10):\n",
    "    DIR = TRAINPATH+\"c\"+str(i)\n",
    "    print (DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def makeDir(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "        return 1\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "            return -1\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dir for sample test\n",
    "DIR = SAMPLETESTPATH\n",
    "print (makeDir(DIR), DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n",
    "\n",
    "for i in range(0, 10):\n",
    "    DIR = SAMPLETESTPATH+\"c\"+str(i)\n",
    "    print (makeDir(DIR), DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dir for validation\n",
    "DIR = VALIDPATH\n",
    "print (makeDir(DIR), DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n",
    "\n",
    "for i in range(0, 10):\n",
    "    DIR = VALIDPATH+\"c\"+str(i)\n",
    "    print (makeDir(DIR), DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move a % of the training data to validation directories.\n",
    "validFract = 0.33\n",
    "for i in range(0, 10):\n",
    "    TRAINDIR = TRAINPATH+\"c\"+str(i)+\"/\"\n",
    "    trainDirFiles = [name for name in os.listdir(TRAINDIR) if os.path.isfile(os.path.join(TRAINDIR, name))]\n",
    "    numToMove = int(validFract*len(trainDirFiles))\n",
    "    print (TRAINDIR, \":\", len(trainDirFiles), numToMove, len(trainDirFiles)-numToMove)\n",
    "    VALIDDIR = VALIDPATH+\"c\"+str(i)+\"/\"\n",
    "    validDirFiles = [name for name in os.listdir(VALIDDIR) if os.path.isfile(os.path.join(VALIDDIR, name))]\n",
    "    print (VALIDDIR, \":\", len(validDirFiles))\n",
    "    if len(validDirFiles)==0:\n",
    "        #select a random validFract of files from TRAINDIR & move to VALIDDIR IFF not already done.\n",
    "        moveList = random.sample(trainDirFiles, numToMove)\n",
    "        print (\"moving \", len(moveList))\n",
    "        for file in moveList:\n",
    "            #print(TRAINDIR+file, VALIDDIR+file)\n",
    "            move(TRAINDIR+file, VALIDDIR+file)\n",
    "            #break\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create sample directories for training, validation and copy a % of files across.\n",
    "#todo: fix overlap sampling error.\n",
    "#select random files and copy to sample.\n",
    "#select random files from sample and _MOVE_ to sample/train\n",
    "#select random files from sample and MOVE to sample/valid\n",
    "#move remaining files to sample/test  now have zero overlap between train/valid/test.\n",
    "\n",
    "DIR = SAMPLEPATH\n",
    "print (makeDir(DIR), DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n",
    "\n",
    "DIR = SAMPLEVALIDPATH \n",
    "print (makeDir(DIR), DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n",
    "\n",
    "DIR = SAMPLETRAINPATH\n",
    "print (makeDir(DIR), DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n",
    "\n",
    "\n",
    "for i in range(0, 10):\n",
    "    DIR = SAMPLEVALIDPATH+\"c\"+str(i)\n",
    "    print (makeDir(DIR), DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n",
    "\n",
    "\n",
    "for i in range(0, 10):\n",
    "    DIR = SAMPLETRAINPATH+\"c\"+str(i)\n",
    "    print (makeDir(DIR), DIR, \":\", len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NBBBB: __IMPORTANT___ this does not create test data correctly, potential overlap between test/train/validate.\n",
    "# Need to segregate randomly selected data properly - maybe start again. UGLY algo below!!!!\n",
    "\n",
    "sampleFract = 0.1\n",
    "\n",
    "for i in range(0, 10):\n",
    "    sampleTrainPath = SAMPLETRAINPATH+\"c\"+str(i)+\"/\"\n",
    "    sampleTrainPathFiles = [name for name in os.listdir(sampleTrainPath) if os.path.isfile(os.path.join(sampleTrainPath, name))]\n",
    "    print (sampleTrainPath, \":\", len(sampleTrainPathFiles))\n",
    "    sampleValidPath = SAMPLEVALIDPATH+\"c\"+str(i)+\"/\"\n",
    "    sampleValidPathFiles = [name for name in os.listdir(sampleValidPath) if os.path.isfile(os.path.join(sampleValidPath, name))]\n",
    "    print (sampleValidPath, \":\", len(sampleValidPathFiles))\n",
    "    sampleTestPath = SAMPLETESTPATH+\"c\"+str(i)+\"/\"\n",
    "    sampleTestPathFiles = [name for name in os.listdir(sampleTestPath) if os.path.isfile(os.path.join(sampleTestPath, name))]\n",
    "    print (sampleTestPath, \":\", len(sampleTestPathFiles))\n",
    "    \n",
    "    TRAINDIR = TRAINPATH+\"c\"+str(i)+\"/\"\n",
    "    trainDirFiles = [name for name in os.listdir(TRAINDIR) if os.path.isfile(os.path.join(TRAINDIR, name))]\n",
    "    numToMoveTrain = int(sampleFract*len(trainDirFiles))\n",
    "    print (TRAINDIR, \":\", len(trainDirFiles), numToMoveTrain, len(trainDirFiles)-numToMoveTrain)\n",
    "\n",
    "    VALIDDIR = VALIDPATH+\"c\"+str(i)+\"/\"\n",
    "    validDirFiles = [name for name in os.listdir(VALIDDIR) if os.path.isfile(os.path.join(VALIDDIR, name))]\n",
    "    numToMoveValid = int(sampleFract*len(validDirFiles))\n",
    "    print (VALIDDIR, \":\", len(validDirFiles), numToMoveValid, len(validDirFiles)-numToMoveValid)\n",
    "    \n",
    "    \n",
    "    #now _COPY_ randomly selected files from train to sample/train    \n",
    "    if len(sampleTrainPathFiles)==0:\n",
    "        copyList = random.sample(trainDirFiles, numToMoveTrain)\n",
    "        print (\"copying \", len(copyList))\n",
    "        for file in copyList:\n",
    "            #print(TRAINDIR+file, sampleTrainPath+file)\n",
    "            copyfile(TRAINDIR+file, sampleTrainPath+file)\n",
    "            #break\n",
    "    print ()\n",
    "    \n",
    "\n",
    "    #now _COPY_ randomly selected files from valid to sample/valid \n",
    "    if len(sampleValidPathFiles)==0:\n",
    "        copyList = random.sample(validDirFiles, numToMoveValid)\n",
    "        print (\"copying \", len(copyList))\n",
    "        for file in copyList:\n",
    "            #print(VALIDDIR+file, sampleValidPath+file)\n",
    "            copyfile(VALIDDIR+file, sampleValidPath+file)\n",
    "            #break\n",
    "    print ()\n",
    "    #break\n",
    "    \n",
    "\n",
    "\n",
    "    #now _COPY_ randomly selected files from train to sample/test    \n",
    "    if len(sampleTestPathFiles)==0:\n",
    "        copyList = random.sample(trainDirFiles, numToMoveTrain)\n",
    "        print (\"copying \", len(copyList))\n",
    "        for file in copyList:\n",
    "            #print(TRAINDIR+file, sampleTestPath+file)\n",
    "            copyfile(TRAINDIR+file, sampleTestPath+file)\n",
    "            #break\n",
    "    print ()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batches = get_batches(SAMPLETRAINPATH, batch_size=batch_size)\n",
    "val_batches = get_batches(SAMPLEVALIDPATH, batch_size=batch_size*2, shuffle=False)\n",
    "print (SAMPLETRAINPATH, type(batches))\n",
    "print (SAMPLEVALIDPATH, type(val_batches))\n",
    "\n",
    "#https://keras.io/preprocessing/image/\n",
    "#https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/DirectoryIterator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames,\n",
    "    test_filename) = get_classes(SAMPLEPATH)\n",
    "print (\"val_classes\", type(val_classes), val_classes.shape)\n",
    "print (\"trn_classes\", type(trn_classes), trn_classes.shape)\n",
    "print (\"val_labels\", type(val_labels), val_labels.shape)\n",
    "print (\"val_labels\", type(val_labels), val_labels.shape)\n",
    "print (\"trn_labels\", type(trn_labels), trn_labels.shape)\n",
    "print (\"val_filenames\", type(val_filenames), len(val_filenames))\n",
    "print (\"filenames\", type(filenames), len(filenames))\n",
    "print (\"test_filename\", type(test_filename), len(test_filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Basic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we try the simplest model and use default parameters. Note the trick of making the first layer a batchnorm layer - that way we don't have to worry about normalizing the input ourselves.  \n",
    "\n",
    "simplest possible liner model  \n",
    "- has single dense layer\n",
    "- start with BatchNormalization layer. normalises the data for us. dont need to calculate the average, standard deviation etc.\n",
    "- flatten layer required to convert images into a single vector first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "#NB: this above is most minimal and simplest model.\n",
    "print (type(model))\n",
    "print (len(model.layers))\n",
    "for layer in model.layers:\n",
    "    print (type(layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see below, this training is going nowhere..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "startTime= datetime.now()\n",
    "print (\"model.compile start\")\n",
    "#https://keras.io/optimizers/#adam\n",
    "#Adam: A Method for Stochastic Optimization\n",
    "model.compile(Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print (\"model.fit_generator start\")\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=6, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)\n",
    "#nb: original epoch was 2\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "video was 7seconds per epoch. mine is 119seconds. loss/accuracy/score is not improving over 2 epochs and low accuracy on both train and validation.\n",
    "\n",
    "Let's first check the number of parameters to see that there's enough parameters to find some useful relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Over 1.5 million parameters - that should be enough. Incidentally, it's worth checking you understand why this is the number of parameters in this layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "10*3*224*224\n",
    "#NB: 10*3*224*224 is not an exact match to the output from model.summary 1505280 vs 1505290\n",
    "\n",
    "#low accuracy probably because learning rate is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since we have a simple model with no regularization and plenty of parameters, it seems most likely that our learning rate is too high. Perhaps it is jumping to a solution where it predicts one or two classes with high confidence, so that it can give a zero prediction to as many classes as possible - that's the best approach for a model that is no better than random, and there is likely to be where we would end up with a high learning rate. So let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(batches))\n",
    "#recall get_batches is from utils.py \n",
    "#uses gen.flow_from_directory(dirname, target_size=target_size,\n",
    "#            class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#np.round(model.predict_generator(batches, batches.N)[:10],2)\n",
    "np.round(model.predict_generator(batches, batches.nb_sample)[:10],2)\n",
    "\n",
    "#first run : got 9 sixes and one 5.\n",
    "#second run:  got 8 sevens and two 6's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our hypothesis was correct. It's nearly always predicting class 1 or 6, with very high confidence. So let's try a lower learning rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "print (len(model.layers))\n",
    "for layer in model.layers:\n",
    "    print (type(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"model.compile start\")\n",
    "model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print (\"model.fit_generator start\")\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Great - we found our way out of that hole... Now we can increase the learning rate and see where we can get to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"model.fit_generator start\")\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=8, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)\n",
    "#epoch was 4\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elpased (hh:mm:ss.ms) {}'.format(timeElapsed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We're stabilizing at validation accuracy of 0.39. Not great, but a lot better than random. Before moving on, let's check that our validation set on the sample is large enough that it gives consistent results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rnd_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=True)\n",
    "#get_batches is from utlils\n",
    "#uses  image.ImageDataGenerator.flow_from_directory \n",
    "#https://keras.io/preprocessing/image/\n",
    "#Takes the path to a directory, and generates batches of augmented/normalized data. \n",
    "#Yields batches indefinitely, in an infinite loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to evaluate if model size is large enough?  \n",
    "use randomly sampled batches.\n",
    "evaluate the range of accuracies resulting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "val_res = [model.evaluate_generator(rnd_batches, rnd_batches.nb_sample) for i in range(10)]\n",
    "np.round(val_res, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(val_res), len(val_res), type(val_res[0]), len(val_res[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get max/min/std-dev of val_res\n",
    "print (\"min:\", [round(elem, 2) for elem in min(val_res)] )\n",
    "print (\"max:\", [round(elem, 2) for elem in max(val_res)] )\n",
    "#print (np.array(val_res).shape)\n",
    "print (\"std:\", np.std(np.array(val_res), axis=0))  \n",
    "#print (std(val_res) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Yup, pretty consistent - if we see improvements of 3% or more, it's probably not random, based on the above samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The previous model is over-fitting a lot, but we can't use dropout since we only have one layer. We can try to decrease overfitting in our model by adding [l2 regularization](http://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html/2) (i.e. add the sum of squares of the weights to our loss function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax', W_regularizer=l2(0.01))\n",
    "    ])\n",
    "model.compile(Adam(lr=10e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looks like we can get a bit over 50% accuracy this way. This will be a good benchmark for our future models - if we can't beat 50%, then we're not even beating a linear model trained on a sample, so we'll know that's not a good approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Single hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next simplest model is to add a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)\n",
    "\n",
    "model.optimizer.lr = 0.01\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Not looking very encouraging... which isn't surprising since we know that CNNs are a much better choice for computer vision problems. So we'll try one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Single conv layer\n",
    "\n",
    "video @ 1:04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2 conv layers with max pooling followed by a simple dense network is a good simple CNN to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),                    #1st convolutional layers\n",
    "            BatchNormalization(axis=1),                                  #followed by BatchNormalization & MaxPooling\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),                    #2nd convolutional layers\n",
    "            BatchNormalization(axis=1),                                  #followed by BatchNormalization & MaxPooling\n",
    "            MaxPooling2D((3,3)), \n",
    "            Flatten(),                                                   #Flattens the input. Does not affect the batch size.\n",
    "            Dense(200, activation='relu'),                               #Dense followed by BatchNormalization\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')                              #NB: output layer is Dense w softmax\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=2, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    model.optimizer.lr = 0.001\n",
    "    model.fit_generator(batches, batches.nb_sample, nb_epoch=4, validation_data=val_batches, \n",
    "                     nb_val_samples=val_batches.nb_sample)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "noted from results above  \n",
    "- training accuracy very quickly reaches 100%\n",
    "- validation accuracy much lower (video was 24%, have 37% here)\n",
    "- CLASSIC OVERFITTING!!!!!\n",
    "\n",
    "The training set here is very rapidly reaching a very high accuracy. So if we could regularize this, perhaps we could get a reasonable result.\n",
    "\n",
    "So, what kind of regularization should we try first? As we discussed in lesson 3, we should start with data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To find the best data augmentation parameters, we can try each type of data augmentation, one at a time. For each type, we can try four very different levels of augmentation, and see which is the best. In the steps below we've only kept the single best result we found. We're using the CNN we defined above, since we have already observed it can model the data quickly and accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Width shift: move the image left and right -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Height shift: move the image up and down -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(height_shift_range=0.05)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Random shear angles (max in radians) -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(shear_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rotation: max in degrees -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Channel shift: randomly changing the R,G,B colors - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(channel_shift_range=20)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And finally, putting it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "batches = get_batches(path+'train', gen_t, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At first glance, this isn't looking encouraging, since the validation set is poor and getting worse. But the training set is getting better, and still has a long way to go in accuracy - so we should try annealing our learning rate and running more epochs, before we make a decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=5, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lucky we tried that - we starting to make progress! Let's keep going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(batches, batches.nb_sample, nb_epoch=25, validation_data=val_batches, \n",
    "                 nb_val_samples=val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "Amazingly, using nothing but a small sample, a simple (not pre-trained) model with no dropout, and data augmentation, we're getting results that would get us into the top 50% of the competition! This looks like a great foundation for our futher experiments.\n",
    "\n",
    "To go further, we'll need to use the whole dataset, since dropout and data volumes are very related, so we can't tweak dropout without using all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"Week4_statefarm-sample-working_saved_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"Week4_statefarm-sample-working.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del loaded_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "json_file = open('Week4_statefarm-sample-working.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(loaded_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loaded_model.load_weights(\"Week4_statefarm-sample-working_saved_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in loaded_model.layers:\n",
    "    print (layer)\n",
    "    print (\"layer.get_weights:\", len(layer.get_weights()))\n",
    "    if len(layer.get_weights())>1:\n",
    "        weights = layer.get_weights()[0]\n",
    "        biases  = layer.get_weights()[1]\n",
    "        print (\"weights:\", weights.shape)\n",
    "        print (\"biases:\", biases.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example submit format.  \n",
    "csv file format.  \n",
    "header row  \n",
    "single decimal point probability for each category  \n",
    "filename.  \n",
    "\n",
    "img,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9  \n",
    "img_1.jpg,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1  \n",
    "img_10.jpg,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1  \n",
    "img_100.jpg,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "nav_menu": {
    "height": "148px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
