{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_or_prod = True #True = sample, False = production\n",
    "\n",
    "batch_size = 64 #ajust to suit memory capacity of hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5110)\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING_DATA: /home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/\n",
      "WORKING_TEST: /home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/test/\n",
      "WORKING_TRAIN: /home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/train/\n",
      "WORKING_VALID: /home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/valid/\n",
      "s_or_p: _sample_\n"
     ]
    }
   ],
   "source": [
    "HOMEPATH = \"/home/ubuntu/fastai/\"\n",
    "\n",
    "DATA_PATH = HOMEPATH+\"data/Kaggle_dogs-vs-cats-redux-kernels-edition/\"\n",
    "\n",
    "MODEL_PATH = DATA_PATH+\"models/\"\n",
    "RESULTS_PATH = DATA_PATH+\"results/\"\n",
    "\n",
    "\n",
    "SAMPLE_DATA_PATH = DATA_PATH + \"sample/\"#choose this for testing or above for production\n",
    "SAMPLE_TEST_PATH = SAMPLE_DATA_PATH+\"test/\"\n",
    "SAMPLE_TRAIN_PATH = SAMPLE_DATA_PATH + \"train/\"\n",
    "SAMPLE_VALID_PATH = SAMPLE_DATA_PATH + \"valid/\"\n",
    "\n",
    "\n",
    "TEST_PATH = DATA_PATH+\"test/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train/\"\n",
    "VALID_PATH = DATA_PATH + \"valid/\"\n",
    "\n",
    "\n",
    "if sample_or_prod:\n",
    "    WORKING_DATA  = SAMPLE_DATA_PATH\n",
    "    WORKING_TEST  = SAMPLE_TEST_PATH\n",
    "    WORKING_TRAIN = SAMPLE_TRAIN_PATH\n",
    "    WORKING_VALID = SAMPLE_VALID_PATH\n",
    "    s_or_p = \"_sample_\"\n",
    "else:\n",
    "    WORKING_DATA  = DATA_PATH\n",
    "    WORKING_TEST  = TEST_PATH\n",
    "    WORKING_TRAIN = TRAIN_PATH\n",
    "    WORKING_VALID = VALID_PATH\n",
    "    s_or_p = \"_prod_\"\n",
    "\n",
    "    \n",
    "print \"WORKING_DATA:\", WORKING_DATA\n",
    "print \"WORKING_TEST:\", WORKING_TEST\n",
    "print \"WORKING_TRAIN:\", WORKING_TRAIN\n",
    "print \"WORKING_VALID:\", WORKING_VALID\n",
    "print \"s_or_p:\", s_or_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "from shutil import copyfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('os.getcwd:', '/home/ubuntu/fastai')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "os.chdir(HOMEPATH)\n",
    "print (\"os.getcwd:\", os.getcwd())\n",
    "# Rather than importing everything manually, we'll make things easy\n",
    "#   and load them all in utils.py, and just import them from there.\n",
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition\n",
      "\u001b[01;34m.\u001b[00m\n",
      "├── \u001b[01;34mmodels\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain_convlayer_features_prod_.bc\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   │   └── \u001b[01;34mmeta\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain_convlayer_features_sample_.bc\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   │   └── \u001b[01;34mmeta\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain_data_prod_.bc\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   │   └── \u001b[01;34mmeta\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain_data_sample_.bc\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   │   └── \u001b[01;34mmeta\u001b[00m\n",
      "│   ├── \u001b[01;34mvalid_convlayer_features_prod_.bc\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   │   └── \u001b[01;34mmeta\u001b[00m\n",
      "│   ├── \u001b[01;34mvalid_convlayer_features_sample_.bc\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   │   └── \u001b[01;34mmeta\u001b[00m\n",
      "│   ├── \u001b[01;34mvalid_data_prod_.bc\u001b[00m\n",
      "│   │   ├── \u001b[01;34mdata\u001b[00m\n",
      "│   │   └── \u001b[01;34mmeta\u001b[00m\n",
      "│   └── \u001b[01;34mvalid_data_sample_.bc\u001b[00m\n",
      "│       ├── \u001b[01;34mdata\u001b[00m\n",
      "│       └── \u001b[01;34mmeta\u001b[00m\n",
      "├── \u001b[01;34mresults\u001b[00m\n",
      "├── \u001b[01;34msample\u001b[00m\n",
      "│   ├── \u001b[01;34mtest\u001b[00m\n",
      "│   │   └── \u001b[01;34munknown\u001b[00m\n",
      "│   ├── \u001b[01;34mtrain\u001b[00m\n",
      "│   │   ├── \u001b[01;34mcat\u001b[00m\n",
      "│   │   └── \u001b[01;34mdog\u001b[00m\n",
      "│   └── \u001b[01;34mvalid\u001b[00m\n",
      "│       ├── \u001b[01;34mcat\u001b[00m\n",
      "│       └── \u001b[01;34mdog\u001b[00m\n",
      "├── \u001b[01;34mtest\u001b[00m\n",
      "│   └── \u001b[01;34munknown\u001b[00m\n",
      "├── \u001b[01;34mtrain\u001b[00m\n",
      "│   ├── \u001b[01;34mcat\u001b[00m\n",
      "│   └── \u001b[01;34mdog\u001b[00m\n",
      "└── \u001b[01;34mvalid\u001b[00m\n",
      "    ├── \u001b[01;34mcat\u001b[00m\n",
      "    └── \u001b[01;34mdog\u001b[00m\n",
      "\n",
      "43 directories\n"
     ]
    }
   ],
   "source": [
    "os.chdir(DATA_PATH)\n",
    "print (os.getcwd())\n",
    "!tree -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = ['cat/', 'dog/']#nb: do not change order or trailing slash\n",
    "UNKNOWN = 'unknown/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_LIST = [DATA_PATH, SAMPLE_DATA_PATH, SAMPLE_TEST_PATH, SAMPLE_TEST_PATH+UNKNOWN, SAMPLE_TRAIN_PATH, \n",
    "                 SAMPLE_VALID_PATH, TEST_PATH, TEST_PATH+UNKNOWN, TRAIN_PATH, VALID_PATH, MODEL_PATH, RESULTS_PATH]\n",
    "\n",
    "\n",
    "\n",
    "for category in CATEGORIES:\n",
    "    DATA_DIR_LIST.append(TRAIN_PATH+category)\n",
    "    DATA_DIR_LIST.append(VALID_PATH+category)\n",
    "    DATA_DIR_LIST.append(SAMPLE_TRAIN_PATH+category)\n",
    "    DATA_DIR_LIST.append(SAMPLE_VALID_PATH+category)\n",
    "\n",
    "\n",
    "#delete the variables we don't need to prevent accidental use.\n",
    "del SAMPLE_DATA_PATH\n",
    "del SAMPLE_TEST_PATH\n",
    "del SAMPLE_TRAIN_PATH\n",
    "del SAMPLE_VALID_PATH\n",
    "del TEST_PATH\n",
    "del TRAIN_PATH\n",
    "del VALID_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirFileList(dir_path):\n",
    "    return [name for name in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listDirsFileCount(DATA_DIR_LIST):\n",
    "    DATA_DIR_LIST = sorted(DATA_DIR_LIST)\n",
    "    for dir_ in DATA_DIR_LIST:\n",
    "        print (dir_, len(dirFileList(dir_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showLayersInfo(model):\n",
    "    print (\"Number of layers : \", len(model.layers))\n",
    "    count = 0\n",
    "    for layer in model.layers:\n",
    "        print (count, type(layer), \", trainable:\", layer.trainable)\n",
    "        print (\"input:\", layer.input_shape, \", output:\",layer.output_shape, \"\\n\")\n",
    "        count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validate'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validate'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/ 3\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/models/ 16\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/results/ 1\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/ 0\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/test/ 0\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/test/unknown/ 2500\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/train/ 0\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/train/cat/ 1750\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/train/dog/ 1750\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/valid/ 0\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/valid/cat/ 750\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/valid/dog/ 750\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/test/ 0\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/test/unknown/ 12500\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/train/ 0\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/train/cat/ 8750\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/train/dog/ 8750\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/valid/ 0\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/valid/cat/ 3750\n",
      "/home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/valid/dog/ 3750\n"
     ]
    }
   ],
   "source": [
    "listDirsFileCount(DATA_DIR_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3500 images belonging to 2 classes.\n",
      "Found 1500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_batches = get_batches(WORKING_TRAIN, shuffle=False, batch_size=batch_size)\n",
    "val_batches = get_batches(WORKING_VALID, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_batches: <class 'keras.preprocessing.image.DirectoryIterator'>\n",
      "val_batches: <class 'keras.preprocessing.image.DirectoryIterator'>\n"
     ]
    }
   ],
   "source": [
    "print (\"trn_batches:\", type(trn_batches))\n",
    "print (\"val_batches:\", type(val_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING_DATA: /home/ubuntu/fastai/data/Kaggle_dogs-vs-cats-redux-kernels-edition/sample/\n",
      "Found 3500 images belonging to 2 classes.\n",
      "Found 1500 images belonging to 2 classes.\n",
      "Found 2500 images belonging to 1 classes.\n",
      "val_classes: <type 'numpy.ndarray'> (1500,) [0 0 0 0 0]\n",
      "trn_classes: <type 'numpy.ndarray'> (3500,) [0 0 0 0 0]\n",
      "val_labels: <type 'numpy.ndarray'> (1500, 2) \n",
      " [[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "trn_labels: <type 'numpy.ndarray'> (3500, 2) \n",
      " [[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "val_filenames: <type 'list'> 1500\n",
      "filenames: <type 'list'> 3500\n",
      "test_filenames: <type 'list'> 2500\n"
     ]
    }
   ],
   "source": [
    "print (\"WORKING_DATA:\", WORKING_DATA)\n",
    "(val_classes, trn_classes, val_labels, trn_labels, val_filenames, filenames, test_filenames) = get_classes(WORKING_DATA)\n",
    "#utils.get_classes\n",
    "#return (val_batches.classes, batches.classes, onehot(val_batches.classes), onehot(batches.classes),\n",
    "#        val_batches.filenames, batches.filenames, test_batches.filenames)\n",
    "#NB: get_classes expects data in subdirectories 'train', 'valid', 'test'\n",
    "print (\"val_classes:\", type(val_classes), val_classes.shape, val_classes[0:5])\n",
    "print (\"trn_classes:\", type(trn_classes), trn_classes.shape, trn_classes[0:5])\n",
    "print (\"val_labels:\", type(val_labels), val_labels.shape, \"\\n\", val_labels[0:5])\n",
    "print (\"trn_labels:\", type(trn_labels), trn_labels.shape, \"\\n\",trn_labels[0:5])\n",
    "print (\"val_filenames:\", type(val_filenames), len(val_filenames))\n",
    "print (\"filenames:\", type(filenames), len(filenames))\n",
    "print (\"test_filenames:\", type(test_filenames), len(test_filenames))\n",
    "\n",
    "#NBB: val_labels = onehot(val_classes) & trn_labels = onehot(trn_classes). onehot 'converts' from 1 to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Vgg16().model\n",
    "#model is stock vgg16 model, all layers trainable, input [3,224,224], output 1,000 classes. \n",
    "#needs to be modified to predict 2 classes.\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showLayersInfo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers,fc_layers = split_at(model, Convolution2D)\n",
    "#utils.split_at(model, layer_type) \n",
    "#splits model at last occurrance of layer_type. (in this case Convolution2D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model        # of layers: 38\n",
      "conv_layers  # of layers: 31\n",
      "fc_layers    # of layers: 7\n"
     ]
    }
   ],
   "source": [
    "print (\"model        # of layers:\",  len(model.layers))\n",
    "print (\"conv_layers  # of layers:\",  len(conv_layers))\n",
    "print (\"fc_layers    # of layers:\", len(fc_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)\n",
    "#31 layers, all trainable, last layer = Convolution2D\n",
    "#showLayersInfo(conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showLayersInfo(Sequential(fc_layers))\n",
    "#7 layers, all trainable, last layer = Dence, 1000 classes output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startTime: 2017-12-20 10:30:36.991712\n",
      "val_features: (1500, 512, 14, 14)\n",
      "trn_features: (3500, 512, 14, 14)\n",
      "Time elapsed (hh:mm:ss.ms) 0:02:11.220578\n"
     ]
    }
   ],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "val_features = conv_model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "print (\"val_features:\", val_features.shape)\n",
    "trn_features = conv_model.predict_generator(trn_batches, trn_batches.nb_sample)\n",
    "print (\"trn_features:\", trn_features.shape)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print ('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "\n",
    "#https://keras.io/models/sequential/\n",
    "#predict_generator(self, generator, steps=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0)\n",
    "#Returns: A Numpy array of predictions.\n",
    "#sample mode: takes approx 2 minutes to run/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_array(MODEL_PATH + 'train_convlayer_features.bc', trn_features)\n",
    "#save_array(MODEL_PATH + 'valid_convlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trn_features = load_array(MODEL_PATH+'train_convlayer_features.bc')\n",
    "#val_features = load_array(MODEL_PATH+'valid_convlayer_features.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startTime: 2017-12-20 10:32:48.234473\n",
      "Found 3500 images belonging to 2 classes.\n",
      "trn: (3500, 3, 224, 224)\n",
      "Found 1500 images belonging to 2 classes.\n",
      "val: (1500, 3, 224, 224)\n",
      "Time elapsed (hh:mm:ss.ms) 0:00:50.524259\n"
     ]
    }
   ],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "trn = get_data(WORKING_TRAIN)\n",
    "print (\"trn:\", trn.shape)\n",
    "val = get_data(WORKING_VALID)\n",
    "print (\"val:\", val.shape)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print ('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "#sample mode: tales approx 50s to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_array(MODEL_PATH+'train_data.bc', trn)\n",
    "#save_array(MODEL_PATH+'valid_data.bc', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trn = load_array(MODEL_PATH+'train_data.bc')\n",
    "#val = load_array(MODEL_PATH+'valid_data.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#showLayersInfo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@start# of layers: 38\n",
      "@end# of layers: 36\n",
      "Number of layers :  36\n",
      "0 <class 'keras.layers.core.Lambda'> , trainable: True\n",
      "input: (None, 3, 224, 224) , output: (None, 3, 224, 224) \n",
      "\n",
      "1 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 3, 224, 224) , output: (None, 3, 226, 226) \n",
      "\n",
      "2 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 3, 226, 226) , output: (None, 64, 224, 224) \n",
      "\n",
      "3 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 64, 224, 224) , output: (None, 64, 226, 226) \n",
      "\n",
      "4 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 64, 226, 226) , output: (None, 64, 224, 224) \n",
      "\n",
      "5 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: True\n",
      "input: (None, 64, 224, 224) , output: (None, 64, 112, 112) \n",
      "\n",
      "6 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 64, 112, 112) , output: (None, 64, 114, 114) \n",
      "\n",
      "7 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 64, 114, 114) , output: (None, 128, 112, 112) \n",
      "\n",
      "8 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 128, 112, 112) , output: (None, 128, 114, 114) \n",
      "\n",
      "9 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 128, 114, 114) , output: (None, 128, 112, 112) \n",
      "\n",
      "10 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: True\n",
      "input: (None, 128, 112, 112) , output: (None, 128, 56, 56) \n",
      "\n",
      "11 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 128, 56, 56) , output: (None, 128, 58, 58) \n",
      "\n",
      "12 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 128, 58, 58) , output: (None, 256, 56, 56) \n",
      "\n",
      "13 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 256, 56, 56) , output: (None, 256, 58, 58) \n",
      "\n",
      "14 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 256, 58, 58) , output: (None, 256, 56, 56) \n",
      "\n",
      "15 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 256, 56, 56) , output: (None, 256, 58, 58) \n",
      "\n",
      "16 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 256, 58, 58) , output: (None, 256, 56, 56) \n",
      "\n",
      "17 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: True\n",
      "input: (None, 256, 56, 56) , output: (None, 256, 28, 28) \n",
      "\n",
      "18 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 256, 28, 28) , output: (None, 256, 30, 30) \n",
      "\n",
      "19 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 256, 30, 30) , output: (None, 512, 28, 28) \n",
      "\n",
      "20 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 512, 28, 28) , output: (None, 512, 30, 30) \n",
      "\n",
      "21 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 512, 30, 30) , output: (None, 512, 28, 28) \n",
      "\n",
      "22 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 512, 28, 28) , output: (None, 512, 30, 30) \n",
      "\n",
      "23 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 512, 30, 30) , output: (None, 512, 28, 28) \n",
      "\n",
      "24 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: True\n",
      "input: (None, 512, 28, 28) , output: (None, 512, 14, 14) \n",
      "\n",
      "25 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 16, 16) \n",
      "\n",
      "26 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 512, 16, 16) , output: (None, 512, 14, 14) \n",
      "\n",
      "27 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 16, 16) \n",
      "\n",
      "28 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 512, 16, 16) , output: (None, 512, 14, 14) \n",
      "\n",
      "29 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: True\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 16, 16) \n",
      "\n",
      "30 <class 'keras.layers.convolutional.Convolution2D'> , trainable: True\n",
      "input: (None, 512, 16, 16) , output: (None, 512, 14, 14) \n",
      "\n",
      "31 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: True\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 7, 7) \n",
      "\n",
      "32 <class 'keras.layers.core.Flatten'> , trainable: True\n",
      "input: (None, 512, 7, 7) , output: (None, 25088) \n",
      "\n",
      "33 <class 'keras.layers.core.Dense'> , trainable: True\n",
      "input: (None, 25088) , output: (None, 4096) \n",
      "\n",
      "34 <class 'keras.layers.core.Dropout'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "35 <class 'keras.layers.core.Dense'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"@start# of layers:\", len(model.layers))#should be 38 layers before popping\n",
    "model.pop()\n",
    "model.pop()\n",
    "print (\"@end# of layers:\", len(model.layers))#should be 36 layers after popping\n",
    "#showLayersInfo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_2 (Lambda)                (None, 3, 224, 224)   0           lambda_input_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_14 (ZeroPadding2D) (None, 3, 226, 226)   0           lambda_2[0][0]                   \n",
      "                                                                   lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 64, 224, 224)  1792        zeropadding2d_14[0][0]           \n",
      "                                                                   zeropadding2d_14[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_15 (ZeroPadding2D) (None, 64, 226, 226)  0           convolution2d_14[0][0]           \n",
      "                                                                   convolution2d_14[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 64, 224, 224)  36928       zeropadding2d_15[0][0]           \n",
      "                                                                   zeropadding2d_15[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_15[0][0]           \n",
      "                                                                   convolution2d_15[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_16 (ZeroPadding2D) (None, 64, 114, 114)  0           maxpooling2d_6[0][0]             \n",
      "                                                                   maxpooling2d_6[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_16 (Convolution2D) (None, 128, 112, 112) 73856       zeropadding2d_16[0][0]           \n",
      "                                                                   zeropadding2d_16[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_17 (ZeroPadding2D) (None, 128, 114, 114) 0           convolution2d_16[0][0]           \n",
      "                                                                   convolution2d_16[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_17 (Convolution2D) (None, 128, 112, 112) 147584      zeropadding2d_17[0][0]           \n",
      "                                                                   zeropadding2d_17[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_7 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_17[0][0]           \n",
      "                                                                   convolution2d_17[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_18 (ZeroPadding2D) (None, 128, 58, 58)   0           maxpooling2d_7[0][0]             \n",
      "                                                                   maxpooling2d_7[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_18 (Convolution2D) (None, 256, 56, 56)   295168      zeropadding2d_18[0][0]           \n",
      "                                                                   zeropadding2d_18[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_19 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_18[0][0]           \n",
      "                                                                   convolution2d_18[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_19 (Convolution2D) (None, 256, 56, 56)   590080      zeropadding2d_19[0][0]           \n",
      "                                                                   zeropadding2d_19[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_20 (ZeroPadding2D) (None, 256, 58, 58)   0           convolution2d_19[0][0]           \n",
      "                                                                   convolution2d_19[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_20 (Convolution2D) (None, 256, 56, 56)   590080      zeropadding2d_20[0][0]           \n",
      "                                                                   zeropadding2d_20[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_8 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_20[0][0]           \n",
      "                                                                   convolution2d_20[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_21 (ZeroPadding2D) (None, 256, 30, 30)   0           maxpooling2d_8[0][0]             \n",
      "                                                                   maxpooling2d_8[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_21 (Convolution2D) (None, 512, 28, 28)   1180160     zeropadding2d_21[0][0]           \n",
      "                                                                   zeropadding2d_21[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_22 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_21[0][0]           \n",
      "                                                                   convolution2d_21[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_22 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_22[0][0]           \n",
      "                                                                   zeropadding2d_22[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_23 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_22[0][0]           \n",
      "                                                                   convolution2d_22[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_23 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_23[0][0]           \n",
      "                                                                   zeropadding2d_23[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_23[0][0]           \n",
      "                                                                   convolution2d_23[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_24 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_9[0][0]             \n",
      "                                                                   maxpooling2d_9[1][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_24 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_24[0][0]           \n",
      "                                                                   zeropadding2d_24[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_25 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_24[0][0]           \n",
      "                                                                   convolution2d_24[1][0]           \n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolution2d_25 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_25[0][0]           \n",
      "                                                                   zeropadding2d_25[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_26 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_25[0][0]           \n",
      "                                                                   convolution2d_25[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_26 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_26[0][0]           \n",
      "                                                                   zeropadding2d_26[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 512, 7, 7)     0           convolution2d_26[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 25088)         0           maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 4096)          102764544   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 4096)          0           dense_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 4096)          16781312    dropout_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showLayersInfo(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_batches: <class 'keras.preprocessing.image.DirectoryIterator'> trn_batches.nb_sample: 3500\n",
      "val_batches: <class 'keras.preprocessing.image.DirectoryIterator'> val_batches.nb_sample: 1500\n"
     ]
    }
   ],
   "source": [
    "print (\"trn_batches:\", type(trn_batches), \"trn_batches.nb_sample:\", trn_batches.nb_sample)\n",
    "print (\"val_batches:\", type(val_batches), \"val_batches.nb_sample:\", val_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startTime: 2017-12-20 10:38:31.871348\n",
      "ll_val_feat: (1500, 4096)\n",
      "ll_feat: (3500, 4096)\n",
      "Time elapsed (hh:mm:ss.ms) 0:02:11.662453\n"
     ]
    }
   ],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "ll_val_feat = model.predict_generator(val_batches, val_batches.nb_sample)\n",
    "print (\"ll_val_feat:\", ll_val_feat.shape)\n",
    "ll_feat = model.predict_generator(trn_batches, trn_batches.nb_sample)\n",
    "print (\"ll_feat:\", ll_feat.shape)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print ('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "\n",
    "#NB: 64^2 = 4096\n",
    "#sample mode: takes approx 2'11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(MODEL_PATH + 'train_ll_feat.bc', ll_feat)\n",
    "save_array(MODEL_PATH + 'valid_ll_feat.bc', ll_val_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ll_feat = load_array(MODEL_PATH+ 'train_ll_feat.bc')\n",
    "#ll_val_feat = load_array(MODEL_PATH + 'valid_ll_feat.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 1 classes.\n",
      "<type 'numpy.ndarray'> (2500, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "startTime= datetime.now()\n",
    "print (\"startTime:\", startTime)\n",
    "\n",
    "test = get_data(WORKING_TEST)\n",
    "print (type(test), test.shape)\n",
    "\n",
    "timeElapsed=datetime.now()-startTime\n",
    "print('Time elapsed (hh:mm:ss.ms) {}'.format(timeElapsed))\n",
    "\n",
    "#sample mode: takes approx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(MODEL_PATH+'test_data.bc', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = load_array(MODEL_PATH+'test_data.bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ll_layers():\n",
    "    #get_ll_layers:create 3 layers, BatchNormalization + Dropout + Dense\n",
    "    return [ \n",
    "        BatchNormalization(input_shape=(4096,)),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax') \n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_last_layer(i):\n",
    "    #nb: i is used in the filename to save weights.\n",
    "    #get_ll_layers:create 3 layers, BatchNormalization + Dropout + Dense\n",
    "    #set learning rate, train model, set learning rate again, train model.\n",
    "    #pop last three layers from vgg16 model, make all layers non trainable.\n",
    "    #add 3 layers created in get_ll_layers to end of model.\n",
    "    #copy weights from 3 layers just trained to last three layers in vgg16 model.\n",
    "    ll_layers = get_ll_layers()\n",
    "    ll_model = Sequential(ll_layers)\n",
    "    ll_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    ll_model.optimizer.lr=1e-5\n",
    "    history = ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), nb_epoch=12)\n",
    "    print (\"train_last_layer:i:\", i, \", ll_model.optimizer.lr:\", ll_model.optimizer.lr )\n",
    "    plot_history(history)\n",
    "    \n",
    "    ll_model.optimizer.lr=1e-7\n",
    "    ll_model.fit(ll_feat, trn_labels, validation_data=(ll_val_feat, val_labels), nb_epoch=1)\n",
    "    #nb: cannot show history with only one epoch.\n",
    "    print (\"ll_model.optimizer.lr:\", ll_model.optimizer.lr)\n",
    "    ll_model.save_weights(MODEL_PATH+'ll_bn' + i + '.h5')\n",
    "\n",
    "    #create new vgg16 model & pop last 3 layers.\n",
    "    vgg = Vgg16()\n",
    "    model = vgg.model\n",
    "    print(\"train_last_layer, model just created from Vgg16(), # layers = \", len(model.layers))\n",
    "    model.pop(); model.pop(); model.pop()\n",
    "    #\n",
    "    for layer in model.layers: layer.trainable=False\n",
    "    #set all layers to non trainable\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    #create three layers ()\n",
    "    ll_layers = get_ll_layers()\n",
    "    #get_ll_layers:create 3 layers, BatchNormalization + Dropout + Dense, add these layers to model.\n",
    "    for layer in ll_layers: model.add(layer)\n",
    "    #copy the weights from the above trained ll_model to the just added  last 3 layers in model.\n",
    "    for l1,l2 in zip(ll_model.layers, model.layers[-3:]):\n",
    "        l2.set_weights(l1.get_weights())\n",
    "    \n",
    "    #compile model so it is ready for use.\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.save_weights(MODEL_PATH+'bn' + i + '.h5')\n",
    "    print (\"@ end of train_last_layer, # layers = \", len(model.layers))\n",
    "    showLayersInfo(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model(model):\n",
    "    #split model into 2 sets of layers. \n",
    "    #conv_layers = start up to & including the last Convolution2D\n",
    "    #fc_layers   = rest of the model\n",
    "    #returns model, list of layers, int position of last Convolution2D layer. \n",
    "    \n",
    "    layers = model.layers\n",
    "    \n",
    "    #last_conv_idx = index of last Convolution2D layer\n",
    "    last_conv_idx = [index for index,layer in enumerate(layers) \n",
    "                         if type(layer) is Convolution2D][-1]\n",
    "\n",
    "    conv_layers = layers[:last_conv_idx+1]\n",
    "    conv_model = Sequential(conv_layers)\n",
    "    fc_layers = layers[last_conv_idx+1:]\n",
    "    return conv_model, fc_layers, last_conv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc_layers(p, in_shape):\n",
    "    #get_fc_layers creates 9 layers, with nominated input shape, nominated dropout rate\n",
    "    #MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, Dense, BatchNormalization, Dropout, Dense\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=in_shape),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(2, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dense_layers(i, model):\n",
    "    #nb: i is used in the filename to save weights.\n",
    "    \n",
    "    print (\"start: train_dense_layers: i:\", i)\n",
    "    print (\"len(model.layers):\", len(model.layers))\n",
    "    #split model about last Convolutional2D layer\n",
    "    conv_model, fc_layers, last_conv_idx = get_conv_model(model)\n",
    "    #conv_model = Sqeuential(conv_layers) = list of layers, from start up to & including the last Convolution2D layer.\n",
    "    #fc_layers   = rest of the model\n",
    "    \n",
    "    \n",
    "    #shape of last Convolution2D layer output.\n",
    "    conv_shape = conv_model.output_shape[1:]\n",
    "    \n",
    "    ##get_fc_layers creates 9 layers, with nominated input shape, nominated dropout rate\n",
    "    #conv_shape ensures input shape from conv_model can be accepted.\n",
    "    fc_model = Sequential(get_fc_layers(0.5, conv_shape))\n",
    "    #get_fc_layers creates 9 layers, with nominated input shape, nominated dropout rate\n",
    "    #MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, Dense, BatchNormalization, Dropout, Dense\n",
    "\n",
    "    \n",
    "    showLayersInfo(fc_model)\n",
    "    showLayersInfo(Sequential(fc_layers))\n",
    "    print (\"len(fc_model.layers):\", len(fc_model.layers))\n",
    "    print (\"len(fc_layers):\", len(fc_layers))\n",
    "    #copy weights from layers in fc_layers to the newly created fc_model. \n",
    "    for l1,l2 in zip(fc_model.layers, fc_layers): \n",
    "        print(\"l1:\", l1)\n",
    "        print(\"l2:\", l2)\n",
    "        weights = l2.get_weights()\n",
    "        print (\"l2.get_weights():\", len(l2.get_weights()))\n",
    "        print (\"l1.get_weights():\", len(l1.get_weights()))\n",
    "        l1.set_weights(weights)\n",
    "    \n",
    "    \n",
    "    fc_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    fc_model.fit(trn_features, trn_labels, nb_epoch=2, \n",
    "         batch_size=batch_size, validation_data=(val_features, val_labels))\n",
    "\n",
    "    #setup data augmentation.\n",
    "    gen = image.ImageDataGenerator(rotation_range=10, \n",
    "                                   width_shift_range=0.05, \n",
    "                                   width_zoom_range=0.05, \n",
    "                                   zoom_range=0.05, \n",
    "                                   channel_shift_range=10, \n",
    "                                   height_shift_range=0.05, \n",
    "                                   shear_range=0.05, \n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "    batches = gen.flow(trn, trn_labels, batch_size=batch_size)\n",
    "    \n",
    "    val_batches = image.ImageDataGenerator().flow(val, \n",
    "                                                  val_labels, \n",
    "                                                  shuffle=False, \n",
    "                                                  batch_size=batch_size)\n",
    "    \n",
    "    print (\"type(val_batches):\", type(val_batches), \"val_batches.N:\", val_batches.N)\n",
    "\n",
    "    print (\"len(conv_model.layers):\", len(conv_model.layers))\n",
    "    \n",
    "    #set all layers in conv_model to non trainable\n",
    "    for layer in conv_model.layers: \n",
    "        layer.trainable = False\n",
    "\n",
    "    ##get_fc_layers creates 9 layers, with nominated input shape, nominated dropout rate\n",
    "    #conv_shape ensures input shape from conv_model can be accepted.\n",
    "    #add all layers to end of conv_model\n",
    "    for layer in get_fc_layers(0.5, conv_shape): \n",
    "        conv_model.add(layer)\n",
    "    print (\"showLayersInfo(conv_model): after 1. setting layers to non trainable & 2. adding get_fc_layers to conv_model\")\n",
    "    showLayersInfo(conv_model)\n",
    "        \n",
    "    \n",
    "    #copy weights from fc_model.layers to the last 9 layers in conv_model.\n",
    "    #nb: weights in fc_model were trained \n",
    "    for l1,l2 in zip(conv_model.layers[last_conv_idx+1:], fc_model.layers): \n",
    "        l1.set_weights(l2.get_weights())\n",
    "\n",
    "    conv_model.compile(optimizer=Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    conv_model.save_weights(model_path+'no_dropout_bn' + i + '.h5')\n",
    "    \n",
    "    \n",
    "    conv_model.fit_generator(batches, \n",
    "                             samples_per_epoch=batches.N, \n",
    "                             nb_epoch=1, \n",
    "                             validation_data=val_batches, \n",
    "                             nb_val_samples=val_batches.N)\n",
    "    \n",
    "    #now make more of the models trainable.\n",
    "    for layer in conv_model.layers[16:]: layer.trainable = True\n",
    "\n",
    "    print (\"showLayersInfo(conv_model): after 1. copying weights to conv_model from fc_model.layers.\")\n",
    "    print (\"2. fitting, setting layers 16: to trainable.\")\n",
    "    \n",
    "    showLayersInfo(conv_model)\n",
    "\n",
    "\n",
    "    history = conv_model.fit_generator(batches, \n",
    "                             samples_per_epoch=batches.N, \n",
    "                             nb_epoch=8, \n",
    "                             validation_data=val_batches, \n",
    "                             nb_val_samples=val_batches.N)\n",
    "\n",
    "    print (\"history = conv_model.fit_generator(blah....), conv_model.optimizer.lr:\", conv_model.optimizer.lr)\n",
    "    plot_history(history)\n",
    "    \n",
    "    conv_model.optimizer.lr = 1e-7\n",
    "    history = conv_model.fit_generator(batches, \n",
    "                             samples_per_epoch=batches.N, \n",
    "                             nb_epoch=10, \n",
    "                             validation_data=val_batches, \n",
    "                             nb_val_samples=val_batches.N)\n",
    "\n",
    "    print (\"history = conv_model.fit_generator(blah....), conv_model.optimizer.lr:\", conv_model.optimizer.lr)\n",
    "    plot_history(history)\n",
    "    \n",
    "    conv_model.save_weights(model_path + 'aug' + i + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.2625 - acc: 0.5366 - val_loss: 1.3201 - val_acc: 0.4267\n",
      "Epoch 2/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.1726 - acc: 0.5526 - val_loss: 1.3464 - val_acc: 0.4087\n",
      "Epoch 3/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.0954 - acc: 0.5774 - val_loss: 1.3974 - val_acc: 0.3953\n",
      "Epoch 4/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.1023 - acc: 0.5791 - val_loss: 1.4412 - val_acc: 0.3867\n",
      "Epoch 5/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.0688 - acc: 0.5880 - val_loss: 1.4669 - val_acc: 0.3767\n",
      "Epoch 6/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.0710 - acc: 0.5897 - val_loss: 1.4793 - val_acc: 0.3733\n",
      "Epoch 7/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.0637 - acc: 0.6011 - val_loss: 1.4743 - val_acc: 0.3707\n",
      "Epoch 8/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.0222 - acc: 0.6091 - val_loss: 1.4525 - val_acc: 0.3773\n",
      "Epoch 9/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.0187 - acc: 0.5989 - val_loss: 1.4621 - val_acc: 0.3747\n",
      "Epoch 10/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.0123 - acc: 0.6043 - val_loss: 1.4562 - val_acc: 0.3707\n",
      "Epoch 11/12\n",
      "3500/3500 [==============================] - 0s - loss: 1.0076 - acc: 0.6149 - val_loss: 1.4558 - val_acc: 0.3707\n",
      "Epoch 12/12\n",
      "3500/3500 [==============================] - 0s - loss: 0.9906 - acc: 0.6111 - val_loss: 1.4606 - val_acc: 0.3660\n",
      "train_last_layer:i: 0 , ll_model.optimizer.lr: 1e-05\n",
      "['acc', 'loss', 'val_acc', 'val_loss']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f963515bc10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8FeW9x/HPLztZIQv7FvZ9jQhq\nFVxaRRQ3ROtKF1p3q/XWLtYuttfeWq9tXamlaBWsol5csFoVoVVRFhEQUNmUsCUQErKvz/1jDscA\nAQLkZE5Ovu/Xa15nzpk5J78JZL5nnmfmGXPOISIiAhDldwEiIhI+FAoiIhKkUBARkSCFgoiIBCkU\nREQkSKEgIiJBCgWRRjKzWWZ2TyPX3WxmZx7v54g0N4WCiIgEKRRERCRIoSARJdBsc4eZrTSzUjP7\nq5l1MLPXzKzYzN40s3b11j/fzD4xs0Ize8fMBtZbNtLMlgfe9w8g4YCfNcnMVgTe+56ZDTvGmr9r\nZuvNrMDMXjKzzoHXzcz+18zyzGyvma0ysyGBZRPNbE2gtq1m9sNj+oWJHEChIJHoYuAsoB9wHvAa\n8BMgC+///M0AZtYPmAPcGlg2H3jZzOLMLA74P+DvQDrwXOBzCbx3JDAT+B6QATwGvGRm8UdTqJmd\nDvw3cCnQCfgCeCaw+OvAqYHtSAusszuw7K/A95xzKcAQ4O2j+bkih6JQkEj0Z+fcTufcVuDfwAfO\nuY+ccxXAi8DIwHpTgVedc/9yzlUD9wFtgJOAsUAs8IBzrto5NxdYUu9nTAcec8594Jyrdc49AVQG\n3nc0rgBmOueWO+cqgR8D48ysJ1ANpAADAHPOrXXObQ+8rxoYZGapzrk9zrnlR/lzRRqkUJBItLPe\nfHkDz5MD853xvpkD4JyrA7YAXQLLtrr9R4z8ot58D+D2QNNRoZkVAt0C7zsaB9ZQgnc00MU59zbw\nIPAQkGdmM8wsNbDqxcBE4AszW2hm447y54o0SKEgrdk2vJ074LXh4+3YtwLbgS6B1/bpXm9+C/Ab\n51zbelOic27OcdaQhNcctRXAOfcn59xoYBBeM9IdgdeXOOcmA+3xmrmePcqfK9IghYK0Zs8C55rZ\nGWYWC9yO1wT0HvA+UAPcbGaxZnYRMKbee/8CfN/MTgx0CCeZ2blmlnKUNcwBppnZiEB/xG/xmrs2\nm9kJgc+PBUqBCqAu0OdxhZmlBZq99gJ1x/F7EAlSKEir5Zz7FLgS+DOwC69T+jznXJVzrgq4CLgW\nKMDrf3ih3nuXAt/Fa97ZA6wPrHu0NbwJ3AU8j3d00hu4LLA4FS989uA1Me0Gfh9YdhWw2cz2At/H\n65sQOW6mm+yIiMg+OlIQEZEghYKIiAQpFEREJChkoWBmMwOX568+xPLxZlYUGCZghZn9PFS1iIhI\n48SE8LNn4Z2Z8eRh1vm3c27S0XxoZmam69mz53GUJSLS+ixbtmyXcy7rSOuFLBScc4sCl+o3qZ49\ne7J06dKm/lgRkYhmZl8ceS3/+xTGmdnHgREsBx9qJTObbmZLzWxpfn5+c9YnItKq+BkKy4Eezrnh\neBcP/d+hVnTOzXDO5TjncrKyjnj0IyIix8i3UHDO7Q0M/oVzbj4Qa2aZftUjIiKh7Wg+LDPrCOx0\nzjkzG4MXULuP8LYGVVdXk5ubS0VFRZPWGIkSEhLo2rUrsbGxfpciImEoZKFgZnOA8UCmmeUCd+ON\nT49z7lHgEuA6M6vBG874MneMY27k5uaSkpJCz5492X9QS6nPOcfu3bvJzc0lOzvb73JEJAyF8uyj\ny4+w/EG8U1aPW0VFhQKhEcyMjIwM1FkvIofi99lHTUaB0Dj6PYnI4fjWpyASVpyDvVth5yeQtwYs\nCtqkQ2K699imXWC+HUSrP0Yil0KhCRQWFjJ79myuv/76o3rfxIkTmT17Nm3btg1RZdKg6nJvx7/z\nE9ix2nvcuRoqChv3/vjUeiGRvv988LHd/sviU0FHadICKBSaQGFhIQ8//PBBoVBTU0NMzKF/xfPn\nzw91aa2bc1CUG9jpr/oqBAo2gAvcqCw2EdoPgsEXQIch3tR+IERFQ1kBlBcEHvd4036vBR4LNkDZ\nHqgsOnQtUTFeQBwUHO0guSP0OROy+is4xHcKhSZw5513smHDBkaMGEFsbCwJCQm0a9eOdevW8dln\nn3HBBRewZcsWKioquOWWW5g+fTrw1ZAdJSUlnHPOOZxyyim89957dOnShXnz5tGmTRuft6wFqSqD\nvLXeN/6d9b/919tRt+3h7fSHXAQdBnvz7bIh6hBda/Ep0K5Hw8saUlvjHW00FBwHhsuezbBtufda\nbSW88VPI6AMDz4MB50GXUQoI8UXEhcIvX/6ENdv2NulnDuqcyt3nHXIUDu69915Wr17NihUreOed\ndzj33HNZvXp18LTPmTNnkp6eTnl5OSeccAIXX3wxGRkZ+33G559/zpw5c/jLX/7CpZdeyvPPP8+V\nV17ZpNsREZyDoi37N/vsXA27NwCBM5pjk7yd/uCLoOO+b/+DICE1tLVFx0BSpjc1lnNQvB0+nQ9r\nX4Z3/wT/+V9I7QIDJsHASdD9JO+zRZqB/qeFwJgxY/a7DuBPf/oTL774IgBbtmzh888/PygUsrOz\nGTFiBACjR49m8+bNzVZvWKupgs/+CZsWBULgk/2badr1DHz7vyQQAIOhbc9Df/sPN2aQ2hlO+I43\nlRXAZ697AbH8CfjwMa+pqf9E7yii13iITfC7aolgERcKh/tG31ySkpKC8++88w5vvvkm77//PomJ\niYwfP77BK6/j4+OD89HR0ZSXlzdLrWHJOdj+MayYDaue85pe4pK9Hf7QS75q+ukwyGviiSSJ6TDi\ncm+qKoX1b3oBsfYlWPGU93voe5YXEH2/HnnbL76LuFDwQ0pKCsXFxQ0uKyoqol27diQmJrJu3ToW\nL17czNW1ICV5sPJZLwzyPoHoOBhwLoy4AnpNaH1NKHFJMGiyN9VUeUdL616Gda/CJy96v59eE7wm\npv4Tj67ZKlScg5KdULDRm/ZshqjYQLNaVr0pExLS1G9ytJwL+e+slf2VhUZGRgYnn3wyQ4YMoU2b\nNnTo0CG47Oyzz+bRRx9l4MCB9O/fn7Fjx/pYaRja1zy0YjZ8/ga4WugyGs79g9cnkJjud4XhISYO\n+p7pTefeD1s+gLWveEcRn78Odgv0OPmrfoi0rqGrpa7O6wcp2PDVzr9gIxRs8h6ry75a16K+OtPr\nQFGxXwVE/bDYLzwyvpqPjaATL/Y7KSFw8kF5/fk9DSwrhBO/D6f/NKSl2TEON+SbnJwcd+BNdtau\nXcvAgQN9qqjl8f331VDzUHJHGD4Vhn8T2g/wr7aWxjnYsTLQxPQy5K/zXu88yguHgedDZt+j/9y6\nWu903oZ2+ns2QU29JtCoWK9vJ70XZPT2HtOzvce0bl6NZbuhND8w7Wp4vmwXlORDzSGaTuOSjxAg\nmV7/S1SMF0bByQJT1P4TB752lOvBse/cKw9zMoxFQULb/S+Y3Hc6c+/Tod/Xj/7fEzCzZc65nCOt\npyMFaT4l+bDyH2oeakpm0Gm4N53+M9j1uRcO616Bt37lTZn9vT6IgZOg04j9d2hFX+6/wy/Y6J3J\nVfgF1FZ99XNiErzTd9N7QZ8zAjv+wJTW1buu43BSO3lTY1SVNhAeBzwv/BK2LvNec7XH9rs7LoFg\nONRREHy1c9+3Y09u712L0qbejj4xHdq03X/HH5/q64kS+iuU0Kqp8po39jUP1dWoeSiUMvvC127z\npqJcr/9h7cvwn/vh3/d539wz+3pt/YVfev8e+8Qmejv59gNgwMT9d/wpnZtvRxWX5E3teh553bo6\n75t68GijwAsJ57wddvCxDqg3H5yOZr0DXscFLkpMD8ud+7FSKEjT29eksWK213G8r3lo3A1qHmpO\naV3hxO95U+lu71qIda94/QEdh8GgC/Zv8knu0PI6fqOivB1yYrr3LVyOm0JBmk5JPqwKnD20c7Wa\nh8JJUgaMusqbRA5Df6VyfNQ8JBJRFApybPadPaTmIZGI0vJ6QSJAcnIyANu2beOSSy5pcJ3x48dz\n4Km3B3rggQcoKys77DpNriQPnrsWHjsVls6EXqfBFXPhB5/AWb9SIIi0cDpS8FHnzp2ZO3fuMb//\ngQce4MorryQxMbEJqzoE52DF0/D6T72Lk8b/BMZ8V81DIhFGRwpN4M477+Shhx4KPv/FL37BPffc\nwxlnnMGoUaMYOnQo8+bNO+h9mzdvZsiQIQCUl5dz2WWXMXDgQC688ML9xj667rrryMnJYfDgwdx9\n992AN8jetm3bmDBhAhMmTADgjTfeYNy4cYwaNYopU6ZQUlLSNBtYsBGenAzzbvBGG/3+uzD+RwoE\nkQgUeUcKr90JO1Y17Wd2HArn3HvIxVOnTuXWW2/lhhtuAODZZ5/l9ddf5+abbyY1NZVdu3YxduxY\nzj///EPeI/mRRx4hMTGRtWvXsnLlSkaNGhVc9pvf/Ib09HRqa2s544wzWLlyJTfffDP3338/CxYs\nIDMzk127dnHPPffw5ptvkpSUxO9+9zvuv/9+fv7znx/7dtfWwOKHYcFvvfOxz70fRk9rkedei0jj\nRF4o+GDkyJHk5eWxbds28vPzadeuHR07duQHP/gBixYtIioqiq1bt7Jz5046duzY4GcsWrSIm2++\nGYBhw4YxbNiw4LJnn32WGTNmUFNTw/bt21mzZs1+ywEWL17MmjVrOPnkkwGoqqpi3Lhxx75R21fC\nSzd6Hcr9J3pnE6V2PvbPE5EWIfJC4TDf6ENpypQpzJ07lx07djB16lSefvpp8vPzWbZsGbGxsfTs\n2bPBIbOPZNOmTdx3330sWbKEdu3ace211zb4Oc45zjrrLObMmXN8G1JdDgt/593sJTEDpjzhjdLZ\n0i5qEpFjonaAJjJ16lSeeeYZ5s6dy5QpUygqKqJ9+/bExsayYMECvvjii8O+/9RTT2X27NkArF69\nmpUrVwKwd+9ekpKSSEtLY+fOnbz22mvB99Qfsnvs2LG8++67rF+/HoDS0lI+++yzo9uITYvgkZO8\nO3+NuBxu+MC7d7ECQaTViLwjBZ8MHjyY4uJiunTpQqdOnbjiiis477zzGDp0KDk5OQwYcPhTNa+7\n7jqmTZvGwIEDGThwIKNHjwZg+PDhjBw5kgEDBtCtW7dg8xDA9OnTOfvss+ncuTMLFixg1qxZXH75\n5VRWVgJwzz330K9fvyMXX74H/vVzWP6kN+jZ1fO8O3yJSKujobNbof1+X2vmwfw7vNEmT7oRTrsT\n4prhFFcRaVYaOlsOb+92mP9Db4C0jsPgm89C5xF+VyUiPlMotDbOQWUJPPR1b7z8M38J427UYHUi\nAkRQKDjnDnkNgARUV+AKv4Ty3d5NWc77ozdssohIQESEQkJCArt37yYjI0PB0BBXByV5uL3b2V1W\nS0KbJLjmZZ1VJCIHiYhQ6Nq1K7m5ueTn5/tdSvipqfJGMa2tgtg2JKS1p+vg4QoEEWlQyELBzGYC\nk4A859yQw6x3AvA+cJlz7phGh4uNjSU7O/vYCo1UVaXe8BSLH/buqDXxPhj4Db+rEpEwF8ojhVnA\ng8CTh1rBzKKB3wFvhLCO1mf9W/DKrd49eHO+BWf+AhLS/K5KRFqAkIWCc26RmfU8wmo3Ac8DJ4Sq\njlalrMAb2vrj2ZDRB6a9Bj1O8rsqEWlBfOtTMLMuwIXABI4QCmY2HZgO0L1799AX1xKtfh7m/xdU\nFMLXfgin3gGxCX5XJSItjJ8dzQ8AP3LO1R3pjCHn3AxgBnhXNDdDbS1HVakXBiuegs6j4Px50PGQ\nXTgiIoflZyjkAM8EAiETmGhmNc65//OxppYlb613a8z8T70jg9Pu1EVoInJcfNuDOOeCpwuZ2Szg\nFQVCI+27NearP4T4ZLjqReg9we+qRCQChPKU1DnAeCDTzHKBu4FYAOfco6H6uRGvsgRevQ1W/gN6\nfg0ufhxSGr5xj4jI0Qrl2UeXH8W614aqjoiyY7XXXFSwAcb/BE79IURF+12ViEQQNUC3BM7Bslnw\nzzu96w2ufgmyv+Z3VSISgRQK4a5ir3ch2urnodcEuOgvkJzld1UiEqEUCuFs+8dec9GezXD6XXDK\nbRClO6iKSOgoFMKRc7DkcXj9J5CYCde+qiuTRaRZKBTCTUURvHSTd5vMPmfBhY9BUobfVYlIK6FQ\nCCdbl8Fz06AoF876FYy7Sc1FItKsFArhwDn44FF44y5vmOtv/RO6jfG7KhFphRQKfivfA/NuhHWv\nQP+JMPkhSEz3uyoRaaUUCn7asgTmfguKt8M3fgtjr9cd0UTEVwoFP9TVwfsPwlu/hNTO8K3Xoeto\nv6sSEVEoNLuyAnjx+/D56zDwPDj/QWjT1u+qREQAhULz+nKx11xUmg/n/B7GfFfNRSISVhQKzaGu\nDt59AN6+B9p2h2+/AZ1H+l2ViMhBFAqhVroLXvwerH8TBl8I5/3RG9RORCQMKRRCafN/4PnveP0I\n594POd9Sc5GIhDWFQijsWO0Ndb30r5DeC654DjoO9bsqEZEjUig0lfJCWPUcfPQUbF8B0XEw8krv\n+oP4FL+rExFpFIXC8airg83/ho/+DmtfhpoK6DAUzvkfGDpFVyaLSIujUDgWhVtgxWxY8RQUful1\nHI+8yjsy6DzC7+pERI6ZQqGxaiq98Yk+ego2LAAc9BoPZ9wNAyZBbILPBYqIHD+FwpFsX+kFwapn\nvcHr0rrBaT+CEd+Edj38rk5EpEkpFBpSvgdWzfX6CrZ/7HUaDzzPax7KHq97HIhIxFIo7FNXB5sW\nekcFa1+G2krvNNJzfg9DL1GnsYi0CgqFwi+9TuOPnoaiLyGhLYy+xjsq6DTc7+pERJpV6wyF6oqv\nOo03vuO91us0OFOdxiLSurWuUNi+0usnWPksVBRCWncYfycMv1ydxiIitKZQ+OhpmHc9RMfDwEne\ndQXZp6nTWESkntYTCv2+ARPvgyEXq9NYROQQWk8oJGV6N7UREZFDUtuJiIgEKRRERCRIoSAiIkEh\nCwUzm2lmeWa2+hDLJ5vZSjNbYWZLzeyUUNUiIiKNE8ojhVnA2YdZ/hYw3Dk3AvgW8HgIaxERkUYI\nWSg45xYBBYdZXuKcc4GnSYA71LoiItI8fO1TMLMLzWwd8Cre0cKh1pseaGJamp+f33wFioi0Mr6G\ngnPuRefcAOAC4NeHWW+Gcy7HOZeTlZXVfAWKiLQyYXH2UaCpqZeZZfpdi4hIa+ZbKJhZHzOzwPwo\nIB7Y7Vc9IiISwmEuzGwOMB7INLNc4G4gFsA59yhwMXC1mVUD5cDUeh3PIiLig5CFgnPu8iMs/x3w\nu1D9fBEROXph0acgIiLhQaEgIiJBCgUREQlSKIiISJBCQUREghQKIiISpFAQEZGgVhUKtXW6Nk5E\n5HBaTSi8t34XX//fhezcW+F3KSIiYavVhEJWSjzbCiu4afZH1NTW+V2OiEhYajWh0LdDCr+9aAgf\nbi7g92986nc5IiJhqdWEAsCFI7ty+ZjuPLZwI2+u2el3OSIiYadVhQLA3ecNYnDnVG5/7mO2FJT5\nXY6ISFhpdaGQEBvNw1eMos45bpi9nMqaWr9LEhEJG40KBTO7xcxSzfNXM1tuZl8PdXGh0iMjid9f\nMpyVuUX85tW1fpcjIhI2Gnuk8C3n3F7g60A74Crg3pBV1QzOHtKR75ySzZPvf8HLH2/zuxwRkbDQ\n2FCwwONE4O/OuU/qvdZi/eicAYzq3pY7n1/JhvwSv8sREfFdY0NhmZm9gRcKr5tZCtDiT/aPjY7i\nwW+OIi4miuufWk55lfoXRKR1a2wofBu4EzjBOVeGd6/laSGrqhl1btuGBy4byWd5xdw1b7Xf5YiI\n+KqxoTAO+NQ5V2hmVwI/A4pCV1bzOq1fFjdN6MPcZbk8u2SL3+WIiPimsaHwCFBmZsOB24ENwJMh\nq8oHt5zZj5P7ZHDXvNWs2bbX73JERHzR2FCocc45YDLwoHPuISAldGU1v+go44GpI0lrE8sNs5dT\nXFHtd0kiIs2usaFQbGY/xjsV9VUzi8LrV4goWSnxPPjNUXxZUMaPnl+Jl4MiIq1HY0NhKlCJd73C\nDqAr8PuQVeWjMdnp3PGN/sxftYNZ7232uxwRkWbVqFAIBMHTQJqZTQIqnHMR1adQ3/Sv9eLMge35\n7fy1fPTlHr/LERFpNo0d5uJS4ENgCnAp8IGZXRLKwvwUFWXcN2U47VMSuHH2R+wprfK7JBGRZtHY\n5qOf4l2jcI1z7mpgDHBX6MryX9vEOB6+YhT5xZXc9uwK6nQrTxFpBRobClHOubx6z3cfxXtbrOHd\n2vKzSQNZ8Gk+jyzc4Hc5IiIhF9PI9f5pZq8DcwLPpwLzQ1NSeLlqbA+WbN7DH974lFHd2zGud4bf\nJYmIhExjO5rvAGYAwwLTDOfcj0JZWLgwM/77oqH0zEzipjkfkVdc4XdJIiIh0+gmIOfc88652wLT\ni6EsKtwkx8fwyBWjKams5uY5H1FT2+LHAhQRadBhQ8HMis1sbwNTsZkddiwIM5tpZnlm1uAoc2Z2\nhZmtNLNVZvZeYAiNsNW/Ywr3XDCUxRsLeODNz/0uR0QkJA4bCs65FOdcagNTinMu9QifPQs4+zDL\nNwGnOeeGAr/Ga54Ka5eM7sqlOV15cMF6Fnyad+Q3iIi0MCE7g8g5twgoOMzy95xz+64MW4x3lXTY\n+9XkIQzomMIP/rGCrYXlfpcjItKkwuW00m8Drx1qoZlNN7OlZrY0Pz+/Gcs6WEJsNI9cOZqaWscN\nTy+nqkb9CyISOXwPBTObgBcKhzybyTk3wzmX45zLycrKar7iDiE7M4n/uWQYK7YU8t+vrfW7HBGR\nJuNrKJjZMOBxYLJzbreftRytiUM7ce1JPfnbu5uZv2q73+WIiDQJ30LBzLoDLwBXOec+86uO4/GT\niQMZ0a0t/zV3JZt2lfpdjojIcQtZKJjZHOB9oL+Z5ZrZt83s+2b2/cAqPwcygIfNbIWZLQ1VLaES\nFxPFQ1eMIibauP7p5VRU1/pdkojIcbGWdiOZnJwct3RpeOXHgnV5TJu1hMtO6Ma9Fw/zuxwRkYOY\n2TLnXM6R1vO9ozkSTBjQnuvH9+aZJVt4flmu3+WIiBwzhUITue2sfpyYnc5P/28Vn+4o9rscEZFj\nolBoIjHRUfz58pEkx8dy3dPLKKms8bskEZGjplBoQu1TE/jz5SPZvKuUH7+wipbWXyMiolBoYuN6\nZ3D71/vz8sfbeGrxF36XIyJyVBQKIXDdab2Z0D+LX768hoWf+Tssh4jI0VAohEBUlPGny0fSt0MK\n1z+1jNVbi/wuSUSkURQKIZKSEMusaSfQNjGOabOWsKWgzO+SRESOSKEQQh1SE5g17QQqq2u55m8f\nsqe0yu+SREQOS6EQYn07pPD4NSeQu6ec7zy5VENhiEhYUyg0gzHZ6TwwdQTLv9zDLc98RG2dTlUV\nkfCkUGgmE4d24mfnDuL1T3by61fW6BoGEQlLMX4X0Jp8+5RstheW8/h/NtEpLYHvndbb75JERPaj\nUGhmP5k4kB17K/jv19bRMS2BySO6+F2SiEiQQqGZRUUZf7h0OPnFlfzwuY/JSo7npD6ZfpclIgKo\nT8EX8THRzLg6h+zMJL7392Ws27HX75JERACFgm/S2sQya9oYEuOjuXbmErYVlvtdkoiIQsFPndu2\nYda0MZRW1nDt3z6kqLza75JEpJVTKPhsYKdUHrtqNJt2lTL9yaVU1ujiNhHxj0IhDJzUJ5P7pgzn\ng00F3P7sx9Tp4jYR8YnOPgoTk0d0YXtRBfe+to7Obdvwk4kD/S5JRFohhUIY+d6pvdheWM6MRRvp\nlJbAtJOz/S5JRFoZhUIYMTN+ft5gduyt4FevrKFjagLnDO3kd1ki0oqoTyHMREcZf7xsJKO6t+OW\nf6zgw00FfpckIq2IQiEMJcRG8/jVOXRt14bvPrmU9XnFfpckIq2EQiFMtUuK44lpY4iNjuKamUvY\nubfC75JEpBVQKISxbumJzJp2AnvKqpj2tyUUV+jiNhEJLYVCmBvSJY2HrxjFpzuLuf7p5VTV1Pld\nkohEMIVCCzC+f3vuvWgo//58F3e+sFI36BGRkNEpqS3ElJxubC+q4P5/fUantATu+MYAv0sSkQik\nUGhBbjq9D9uLynlowQY6pbXhyrE9/C5JRCKMQqEFMTN+PXkIeXsr+fm81XRITeCsQR38LktEIkjI\n+hTMbKaZ5ZnZ6kMsH2Bm75tZpZn9MFR1RJqY6Cj+/M2RDO2Sxk1zlrP8yz2+1KF+DZHIZKH64zaz\nU4ES4Enn3JAGlrcHegAXAHucc/c15nNzcnLc0qVLm7TWlmhXSSUXP/IexRU1PH/dSWRnJjXJ51bX\n1rGrpJKdeyvZubeCvL0VwfmdxZWB5xVU1tRxaU43vn9abzqmJTTJzxaR0DGzZc65nCOuF8pvfGbW\nE3iloVCot84vgBKFwtHbvKuUix55j+T4GJ6/7iSyUuIPuW5tnWN3vZ39zmJvZ79vJ79zbyV5xZXs\nLq3kwP8SUQZZKfF0SE2gfUoCHVLjKa2s4ZWV24kyY0pOV64b35uu7RJDvMUicqwiKhTMbDowHaB7\n9+6jv/jii6YttAX76Ms9XP6XxfTrkMKtZ/Ylb29gx1/sfcvPK/ZCIL+4kgNv02AGGUnxdEj1dvgd\nUuMDO/2E4GvtU+PJSIonOsoO+tlbCsp4ZOEGnlu6Befg4lFduX5Cb3pkNM1Ri4g0nYgKhfp0pHCw\nt9bu5LtPLt1vp5+eFEf7lPgDdvAJdAi+lkBGchyx0cffrbStsJzHFm5gzpIt1NY5Jo/ozA0T+tA7\nK/m4P1tEmoZCoZXZkF9CUXk1HVITyEqOJy6m+a9LzNtbwWOLNvL0B19QWVPHecM6c+PpfejXIaXZ\naxGR/TU2FHRKaoQIh2/l7VMTuGvSIK4b35vH/72JJ9/fzEsfb+OcIR258fQ+DO6c5neJInIEoTz7\naA4wHsgEdgJ3A7EAzrlHzawjsBRIBerwzlQa5Jzbe7jP1ZFCy7GntIqZ725i1rubKa6s4cyBHbj5\njD4M69rW79JEWp2waD4KBYUVQVc3AAANu0lEQVRCy1NUXs2sdzcz891NFJVXM75/Fjed3pfRPdr5\nXZpIq6FQkLBTXFHNk+9/wV//s4mC0ipO6ZPJTaf34cReGX6XJhLxFAoStsqqanh68Zc8tmgju0oq\nGZOdzi1n9OWk3hmYHXzqq4gcP4WChL2K6lrmfPgljy7cwM69lYzq3pabz+jLaf2yFA4iTUyhIC1G\nRXUtzy3L5dF3NrC1sJxhXdO46fS+nDmwvcJBpIkoFKTFqaqp44XluTz0znq2FJQzqFMqN53eh28M\n7khUA1dUHy3nHJU1dYGplsrqevM1dYHntVTV1JHaJpaOqQl0TEsgITa6CbZOxF8KBWmxqmvrmLdi\nGw8tWM+mXaX065DMpGGdqa1zDe7Egzv66trD7vSP9VamaYGA6JCWQMfUr64I3xcaHVITyEiKa5Lg\nEgkVhYK0eLV1jldWbuPBt9fzeV4JAHExUcTHRBEfE+09xtabj4kiPrbefEx0YPnRrR8XHUVhWTU7\n91awIzBg4I6iiuDzhsaRio224GCB+4KifmjoqEP8piuapcWLjjImj+jC+cM7U1lTR3xMVFj0MdTU\n1rGrpIodB4TFziLvcd2OYhZ+mk9pVe1B7z3wqKNjagLdM5KYNKyTAkPCgkJBwp6ZhdUOMyY6io5p\n3jd/uh16veKKwNFGUeV+Rxz75tdt38uuEu+o48G3P+eXk4dwWr+s5tsQkQYoFERCJCUhlpSEWPq0\nP/SAgDW1dby3YTe/eOkTrpn5IROHduSuSYPolNamGSsV+UrzD6UpIkEx0VGc2i+L1279Gnd8oz9v\nrc3jjD8sZMaiDVTXHlvHuMjxUCiIhIH4mGhumNCHN287jZN6Z/Db+es490//5oONu/0uTVoZhYJI\nGOmWnsjj15zAX67OobSylqkzFnPbP1aQX1zpd2nSSigURMLQWYM68OZtp3HDhN68vHIbp//hHf7+\n/mZqDzwXVqSJKRREwlSbuGju+MYA/nnrqQzrmsZd8z7hgofeZcWWQr9LkwimUBAJc72zknnq2yfy\n58tHsnNvBRc+/C4/eXEVhWVVfpcmEUihINICmBnnDe/MW7efxrdOzuYfS7Zw+h8W8uzSLdSpSUma\nkEJBpAVJSYjlrkmDeOWmU8jOTOK/5q5kymPvs3b7Ye9iK9JoCgWRFmhgp1Se+944/ueSYWzaVcqk\nP/+HX7+yhuKKar9LkxZOoSDSQkVFGZfmdOPt209j6gndmPnuJs74w0Je+ngbLW2gSwkfCgWRFq5t\nYhy/vXAoL15/Mu1T47l5zkdc+dcP2JBf4ndp0gIpFEQixIhubZl3wyn8avJgVuYWcfYDi/j96+so\nb2C0VpFDUSiIRJDoKOPqcT15+/bxnDesMw8t2MCZ9y/kX2t2+l2atBAKBZEIlJUSz/1TR/DM9LEk\nxkXz3SeX8p0nlrCloMzv0iTM6c5rIhGuuraOv727iQfe/JzaOseFI7vQLimO5PgYkuKiSYqP8eYD\nkzcfHXwtNlrfHSOB7rwmIgDERkcx/dTeTBrWmd/MX8v8Vdsprapt9DhKcTFRJMfHkBgXfUB4RJMU\nF7NfqCTHRx8QLjFkJMWRmRxPm7jwuVGSHJpCQaSV6Ny2DQ99cxQAzjkqa+oorayhtLKWksoaSqtq\nvMfAVFJZ681X1ey/XmUNRWVVbN3jvbZvnSNlTEp8DJkp8WQlx5OVUm864HlGUhwxOjrxjUJBpBXa\nd4vThNhoMpKP//Occ5RXe6FRVi88SiprKCitIr+kkvzir6a1O/ay6PNKiitqGqgN0hPjDhka9Z+n\ntYkNi/t2RxKFgogcNzMjMS6GxLgYOPTdRw9SUV3rBUW90NhVP0BKKtm0q5S84kqqag6+E11stJGV\nHB88AslMjqdtUixt28SR1iaWtomxpLWJ3W8+OT5GQXIYCgUR8U1CbDTd0hPplp542PWccxRX1ux3\ntHFgmOzYW8GqrUUUllc3GCD7xERZMCjSEmNpGwyNuIMCxHv86vW4mMhv1lIoiEjYMzNSE2JJTYil\nd9aR27sqqmspLKumqLyawrIqCsu9+aKyagrLq4LLisqr2VVSxYb8UgrLqtjbQHNWfUlx0YEwiaNt\nm1i6tGvDmOx0xvXKoGu7NhFxBBKyUDCzmcAkIM85N6SB5Qb8EZgIlAHXOueWh6oeEWk9EmKj6ZgW\nTce0hKN6X22do7iimsKy6mCQFJZV1QuU6v3C5u11ecxdlgtA57QExvbK4MRe6YztlUH39MQWGRKh\nPFKYBTwIPHmI5ecAfQPTicAjgUcREV9ERxltE+NomxjXqPXr6hzr80tYvHE3H2wsYOFn+bzw0VYA\nOqYmBAPixOx0sjOTWkRIhCwUnHOLzKznYVaZDDzpvKvnFptZWzPr5JzbHqqaRESaUlSU0a9DCv06\npHD1uJ4459iQX8L7Gwv4YONu3l2/m3krtgHQPiWeE3tlMLZXOidmZ9A7KzxDws8+hS7AlnrPcwOv\nHRQKZjYdmA7QvXv3ZilORORomRl92qfQp30KV43tgXOOjbtKg0cSizfu5uWPvZDITI4PHkmMzU6n\nT/vksAiJFtHR7JybAcwAb5gLn8sREWkUM6N3VjK9s5K54kQvJDbvLguExG4Wbyzg1ZXe9+DM5DjG\nZO9rbsqgb/tkoqKaPyT8DIWtQLd6z7sGXhMRiUhmRnZmEtmZSVw+pjvOOb4sKNvvSGL+qh0ApCfF\nMaZnutfc1CuD/h1SmiUk/AyFl4AbzewZvA7mIvUniEhrYmb0yEiiR0YSU0/wQiJ3Tznv1wuJf37i\nhUTbxFhuGN+H757aK6Q1hfKU1DnAeCDTzHKBu4FYAOfco8B8vNNR1+OdkjotVLWIiLQEZha8mO/S\nHK8hZUtBGR9s8jquOxzlKbbHVIOGzhYRiXyNHTo78q/ZFhGRRlMoiIhIkEJBRESCFAoiIhKkUBAR\nkSCFgoiIBCkUREQkSKEgIiJBLe7iNTPLB744xrdnAruasJxwE8nbp21ruSJ5+1rStvVwzmUdaaUW\nFwrHw8yWNuaKvpYqkrdP29ZyRfL2ReK2qflIRESCFAoiIhLU2kJhht8FhFgkb5+2reWK5O2LuG1r\nVX0KIiJyeK3tSEFERA5DoSAiIkGtJhTM7Gwz+9TM1pvZnX7X01TMrJuZLTCzNWb2iZnd4ndNTc3M\nos3sIzN7xe9ampqZtTWzuWa2zszWmtk4v2tqKmb2g8D/ydVmNsfMQn/bsBAys5lmlmdmq+u9lm5m\n/zKzzwOP7fyssSm0ilAws2jgIeAcYBBwuZkN8reqJlMD3O6cGwSMBW6IoG3b5xZgrd9FhMgfgX86\n5wYAw4mQ7TSzLsDNQI5zbggQDVzmb1XHbRZw9gGv3Qm85ZzrC7wVeN6itYpQAMYA651zG51zVcAz\nwGSfa2oSzrntzrnlgflivJ1KF3+rajpm1hU4F3jc71qampmlAacCfwVwzlU55wr9rapJxQBtzCwG\nSAS2+VzPcXHOLQIKDnh5MvBEYP4J4IJmLSoEWksodAG21HueSwTtOPcxs57ASOADfytpUg8A/wXU\n+V1ICGQD+cDfAs1jj5tZkt9FNQXn3FbgPuBLYDtQ5Jx7w9+qQqKDc257YH4H0MHPYppCawmFiGdm\nycDzwK3Oub1+19MUzGwSkOecW+Z3LSESA4wCHnHOjQRKiYDmB4BA2/pkvODrDCSZ2ZX+VhVazju/\nv8Wf499aQmEr0K3e866B1yKCmcXiBcLTzrkX/K6nCZ0MnG9mm/Ga/E43s6f8LalJ5QK5zrl9R3Zz\n8UIiEpwJbHLO5TvnqoEXgJN8rikUdppZJ4DAY57P9Ry31hIKS4C+ZpZtZnF4HV4v+VxTkzAzw2uT\nXuucu9/vepqSc+7HzrmuzrmeeP9mbzvnIubbpnNuB7DFzPoHXjoDWONjSU3pS2CsmSUG/o+eQYR0\noh/gJeCawPw1wDwfa2kSMX4X0BycczVmdiPwOt5ZEDOdc5/4XFZTORm4ClhlZisCr/3EOTffx5qk\n8W4Cng58WdkITPO5nibhnPvAzOYCy/HOkPuIFj4khJnNAcYDmWaWC9wN3As8a2bfxhvS/1L/Kmwa\nGuZCRESCWkvzkYiINIJCQUREghQKIiISpFAQEZEghYKIiAQpFESakZmNj8TRXiVyKBRERCRIoSDS\nADO70sw+NLMVZvZY4J4OJWb2v4F7BLxlZlmBdUeY2WIzW2lmL+4bU9/M+pjZm2b2sZktN7PegY9P\nrncPhacDV/yKhAWFgsgBzGwgMBU42Tk3AqgFrgCSgKXOucHAQrwrWgGeBH7knBsGrKr3+tPAQ865\n4Xjj/uwbTXMkcCvevT164V2VLhIWWsUwFyJH6QxgNLAk8CW+Dd5AZ3XAPwLrPAW8ELgnQlvn3MLA\n608Az5lZCtDFOfcigHOuAiDweR8653IDz1cAPYH/hH6zRI5MoSByMAOecM79eL8Xze46YL1jHSOm\nst58Lfo7lDCi5iORg70FXGJm7SF4H94eeH8vlwTW+SbwH+dcEbDHzL4WeP0qYGHgLni5ZnZB4DPi\nzSyxWbdC5BjoG4rIAZxza8zsZ8AbZhYFVAM34N0EZ0xgWR5evwN4QyY/Gtjp1x/p9CrgMTP7VeAz\npjTjZogcE42SKtJIZlbinEv2uw6RUFLzkYiIBOlIQUREgnSkICIiQQoFEREJUiiIiEiQQkFERIIU\nCiIiEvT/TtmXx+fS6hUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9575f6fe90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3500 samples, validate on 1500 samples\n",
      "Epoch 1/1\n",
      "3500/3500 [==============================] - 0s - loss: 0.9851 - acc: 0.6091 - val_loss: 1.4650 - val_acc: 0.3640\n",
      "ll_model.optimizer.lr: 1e-07\n",
      "train_last_layer, model just created from Vgg16(), # layers =  38\n",
      "@ end of train_last_layer, # layers =  38\n",
      "Number of layers :  38\n",
      "0 <class 'keras.layers.core.Lambda'> , trainable: False\n",
      "input: (None, 3, 224, 224) , output: (None, 3, 224, 224) \n",
      "\n",
      "1 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 3, 224, 224) , output: (None, 3, 226, 226) \n",
      "\n",
      "2 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 3, 226, 226) , output: (None, 64, 224, 224) \n",
      "\n",
      "3 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 64, 224, 224) , output: (None, 64, 226, 226) \n",
      "\n",
      "4 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 64, 226, 226) , output: (None, 64, 224, 224) \n",
      "\n",
      "5 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: False\n",
      "input: (None, 64, 224, 224) , output: (None, 64, 112, 112) \n",
      "\n",
      "6 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 64, 112, 112) , output: (None, 64, 114, 114) \n",
      "\n",
      "7 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 64, 114, 114) , output: (None, 128, 112, 112) \n",
      "\n",
      "8 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 128, 112, 112) , output: (None, 128, 114, 114) \n",
      "\n",
      "9 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 128, 114, 114) , output: (None, 128, 112, 112) \n",
      "\n",
      "10 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: False\n",
      "input: (None, 128, 112, 112) , output: (None, 128, 56, 56) \n",
      "\n",
      "11 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 128, 56, 56) , output: (None, 128, 58, 58) \n",
      "\n",
      "12 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 128, 58, 58) , output: (None, 256, 56, 56) \n",
      "\n",
      "13 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 256, 56, 56) , output: (None, 256, 58, 58) \n",
      "\n",
      "14 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 256, 58, 58) , output: (None, 256, 56, 56) \n",
      "\n",
      "15 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 256, 56, 56) , output: (None, 256, 58, 58) \n",
      "\n",
      "16 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 256, 58, 58) , output: (None, 256, 56, 56) \n",
      "\n",
      "17 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: False\n",
      "input: (None, 256, 56, 56) , output: (None, 256, 28, 28) \n",
      "\n",
      "18 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 256, 28, 28) , output: (None, 256, 30, 30) \n",
      "\n",
      "19 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 256, 30, 30) , output: (None, 512, 28, 28) \n",
      "\n",
      "20 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 512, 28, 28) , output: (None, 512, 30, 30) \n",
      "\n",
      "21 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 512, 30, 30) , output: (None, 512, 28, 28) \n",
      "\n",
      "22 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 512, 28, 28) , output: (None, 512, 30, 30) \n",
      "\n",
      "23 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 512, 30, 30) , output: (None, 512, 28, 28) \n",
      "\n",
      "24 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: False\n",
      "input: (None, 512, 28, 28) , output: (None, 512, 14, 14) \n",
      "\n",
      "25 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 16, 16) \n",
      "\n",
      "26 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 512, 16, 16) , output: (None, 512, 14, 14) \n",
      "\n",
      "27 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 16, 16) \n",
      "\n",
      "28 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 512, 16, 16) , output: (None, 512, 14, 14) \n",
      "\n",
      "29 <class 'keras.layers.convolutional.ZeroPadding2D'> , trainable: False\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 16, 16) \n",
      "\n",
      "30 <class 'keras.layers.convolutional.Convolution2D'> , trainable: False\n",
      "input: (None, 512, 16, 16) , output: (None, 512, 14, 14) \n",
      "\n",
      "31 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: False\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 7, 7) \n",
      "\n",
      "32 <class 'keras.layers.core.Flatten'> , trainable: False\n",
      "input: (None, 512, 7, 7) , output: (None, 25088) \n",
      "\n",
      "33 <class 'keras.layers.core.Dense'> , trainable: False\n",
      "input: (None, 25088) , output: (None, 4096) \n",
      "\n",
      "34 <class 'keras.layers.core.Dropout'> , trainable: False\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "35 <class 'keras.layers.normalization.BatchNormalization'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "36 <class 'keras.layers.core.Dropout'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "37 <class 'keras.layers.core.Dense'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 2) \n",
      "\n",
      "start: train_dense_layers: i: 0\n",
      "len(model.layers): 38\n",
      "Number of layers :  9\n",
      "0 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: True\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 7, 7) \n",
      "\n",
      "1 <class 'keras.layers.core.Flatten'> , trainable: True\n",
      "input: (None, 512, 7, 7) , output: (None, 25088) \n",
      "\n",
      "2 <class 'keras.layers.core.Dense'> , trainable: True\n",
      "input: (None, 25088) , output: (None, 4096) \n",
      "\n",
      "3 <class 'keras.layers.normalization.BatchNormalization'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "4 <class 'keras.layers.core.Dropout'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "5 <class 'keras.layers.core.Dense'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "6 <class 'keras.layers.normalization.BatchNormalization'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "7 <class 'keras.layers.core.Dropout'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "8 <class 'keras.layers.core.Dense'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 2) \n",
      "\n",
      "Number of layers :  7\n",
      "0 <class 'keras.layers.pooling.MaxPooling2D'> , trainable: False\n",
      "input: (None, 512, 14, 14) , output: (None, 512, 7, 7) \n",
      "\n",
      "1 <class 'keras.layers.core.Flatten'> , trainable: False\n",
      "input: (None, 512, 7, 7) , output: (None, 25088) \n",
      "\n",
      "2 <class 'keras.layers.core.Dense'> , trainable: False\n",
      "input: (None, 25088) , output: (None, 4096) \n",
      "\n",
      "3 <class 'keras.layers.core.Dropout'> , trainable: False\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "4 <class 'keras.layers.normalization.BatchNormalization'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "5 <class 'keras.layers.core.Dropout'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 4096) \n",
      "\n",
      "6 <class 'keras.layers.core.Dense'> , trainable: True\n",
      "input: (None, 4096) , output: (None, 2) \n",
      "\n",
      "l1: <keras.layers.pooling.MaxPooling2D object at 0x7f956a71fc10> , l2: <keras.layers.pooling.MaxPooling2D object at 0x7f9635caaad0>\n",
      "l2.get_weights(): 0\n",
      "l1.get_weights(): 0\n",
      "l1: <keras.layers.core.Flatten object at 0x7f956b21e690> , l2: <keras.layers.core.Flatten object at 0x7f956aee3c10>\n",
      "l2.get_weights(): 0\n",
      "l1.get_weights(): 0\n",
      "l1: <keras.layers.core.Dense object at 0x7f956a73d290> , l2: <keras.layers.core.Dense object at 0x7f956aaf0350>\n",
      "l2.get_weights(): 2\n",
      "l1.get_weights(): 2\n",
      "l1: <keras.layers.normalization.BatchNormalization object at 0x7f956a73d090> , l2: <keras.layers.core.Dropout object at 0x7f956ad304d0>\n",
      "l2.get_weights(): 0\n",
      "l1.get_weights(): 4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"batchnormalization_15\" with a  weight list of length 0, but the layer was expecting 4 weights. Provided weights: []...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-df8ec05c33a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#at end of train_last_layer model has ?? layers with last three layers being BatchNormalization + Dropout + Dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_dense_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-497eff5b64c6>\u001b[0m in \u001b[0;36mtrain_dense_layers\u001b[0;34m(i, model)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"l2.get_weights():\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"l1.get_weights():\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    973\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                              \u001b[0;34m' weights. Provided weights: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                              str(weights)[:50] + '...')\n\u001b[0m\u001b[1;32m    976\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"batchnormalization_15\" with a  weight list of length 0, but the layer was expecting 4 weights. Provided weights: []..."
     ]
    }
   ],
   "source": [
    "#build the ensemble\n",
    "#multiple model builds, model trainings. takes long time.\n",
    "for i in range(5):\n",
    "    i = str(i)\n",
    "    print (\"i:\", i)\n",
    "    model = train_last_layer(i)\n",
    "    #a tthis point, model = vgg16 model, minus last three layers, plus layers BatchNormalization + Dropout + Dense\n",
    "    #get_ll_layers:create 3 layers, BatchNormalization + Dropout + Dense\n",
    "    #train_last_layer uses get_ll_layers to create 3 layer model, trains it, then pops last 3 layers from vgg16model\n",
    "    #at end of train_last_layer model has 38 layers with last three layers being BatchNormalization + Dropout + Dense\n",
    "\n",
    "    train_dense_layers(i, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine ensemble and test\n",
    "ens_model = vgg_ft(2)\n",
    "for layer in ens_model.layers: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ens_pred(arr, fname):\n",
    "    ens_pred = []\n",
    "    for i in range(5):\n",
    "        i = str(i)\n",
    "        ens_model.load_weights('{}{}{}.h5'.format(model_path, fname, i))\n",
    "        preds = ens_model.predict(arr, batch_size=batch_size)\n",
    "        ens_pred.append(preds)\n",
    "    return ens_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
